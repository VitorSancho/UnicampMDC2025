---
title:  INF0613 -- Aprendizado de Máquina Não Supervisionado
output: pdf_document
subtitle: Trabalho 2 - Redução de Dimensionalidade e Técnicas de Agrupamento
author: 
  - Nome completo Integrante 1
  - Nome completo Integrante 2
---

<!-- !!!!! Atenção !!!!! -->
<!-- Antes de fazer qualquer alteração neste arquivo, reabra-o com 
o encoding correto: 
File > Reopen with encoding > UTF-8
-->







```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, error = FALSE, message = FALSE, warning = FALSE, tidy = FALSE)
options(digits = 3)
```




O objetivo deste trabalho é exercitar o conhecimento de técnicas de redução de dimensionalidade e técnicas de agrupamento. Usaremos a base de dados `speech.csv`, que está disponível na página da disciplina no Moodle. A base contém amostras da pronúncia em inglês das letras do alfabeto.

# Atividade 0 -- Configurando o ambiente
Antes de começar a implementação do seu trabalho configure o _workspace_ e importe todos os pacotes e execute o preprocessamento da base:

```{r atv0-code}
# Adicione os demais pacotes usados neste trabalho:


# Configure ambiente de trabalho na mesma pasta 
# onde colocou a base de dados:
# setwd("")

# Pré-processamento da base de dados
# Lendo a base de dados
speech <- read.csv("speech.csv", header = TRUE)

# Convertendo a coluna 618 em characteres 
speech$LETRA <- LETTERS[speech$LETRA]

```



# Atividade 1 -- Análise de Componentes Principais (*2,0 pts*)

Durante a redução de dimensionalidade, espera-se que o poder de representação do conjunto de dados seja mantido, para isso é preciso realizar uma análise da variância mantida em cada componente principal obtido. Use função  `prcomp`, que foi vista em aula, para criar os autovetores e autovalores da base de dados. Não use a normalização dos atributos, isto é, defina  `scale.=FALSE`. 
Em seguida, use o comando `summary`, analise o resultado e os itens a seguir:


<!-- Use o comando: options(max.print=2000) para visualizar o resultado 
do comando summary e fazer suas análises. Não é necessário que toda informação 
do comando summary apareça no PDF a ser submetido. Desse modo, repita o comando 
com um valor mais baixo antes de gerar a versão final do PDF. -->

```{r atv1-code}

# Executando a redução de dimensionalidade com o prcomp
dados_para_PCA <- speech[, 1:617]
pca <- prcomp(dados_para_PCA, scale. = FALSE)

# Analisando as componentes com o comando summary
summary(pca)

# Analise de vari^ancia explicada
var_exp <- (pca$sdev^2) / sum(pca$sdev^2)
cum_var_exp <- cumsum(var_exp)
head(cum_var_exp, 20)
```

## Análise

a) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `80%` do total? 
```{r}
which(cum_var_exp >= 0.80)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 80% precisamos de 38 componentes.
<!-- Fim da resposta -->

b) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `90%` do total? 
```{r}
which(cum_var_exp >= 0.90)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 90% precisamos de 91 componentes.
<!-- Fim da resposta -->

c) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `95%` do total? 
```{r}
which(cum_var_exp >= 0.95)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 95% precisamos de 170 componentes.
<!-- Fim da resposta -->

d) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `99%` do total? 
```{r}
which(cum_var_exp >= 0.99)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 99% precisamos de 382 componentes.
<!-- Fim da resposta -->

e) Faça um breve resumo dos resultados dos itens *a)-d)* destacando o impacto da redução de dimensionalidade. 
**Resposta:** <!-- Escreva sua resposta abaixo -->
A análise dos componentes principais mostra que a variabilidade dos dados pode ser representada de forma eficiente por um subconjunto dos componentes. Observa-se que os primeiros 38 componentes capturam aproximadamente 80% da variância total, indicando que grande parte da informação está concentrada nesses poucos componentes iniciais. Ao aumentar o nível de variância desejada, a quantidade de componentes necessários cresce: 91 componentes explicam 90% da variância, 170 componentes explicam 95%, e para atingir 99% da variância, é necessário manter 382 componentes.

Esses resultados evidenciam que, embora seja possível reduzir a dimensionalidade do conjunto de dados, existe um trade-off entre simplificação e preservação da informação. A redução de dimensionalidade ajuda a eliminar redundâncias e ruídos, facilita visualização, diminui custos computacionais e pode melhorar o desempenho de algoritmos de aprendizado de máquina. Por outro lado, se quisermos capturar praticamente toda a variabilidade (99%), a redução se torna menos significativa, pois é necessário manter a maioria das componentes. Portanto, a escolha do número de componentes deve equilibrar a economia de dimensionalidade com a preservação da informação relevante para análise ou modelagem.
<!-- Fim da resposta -->

# Atividade 2 -- Análise de Componentes Principais e Normalização (*2,0 pts*)

A normalização de dados em alguns casos, pode trazer benefícios. Nesta questão, iremos analisar o impacto dessa prática na redução da dimensionalidade da base de dados `speech.csv`. Use função  `prcomp` para criar os autovetores e autovalores da base de dados usando a normalização dos atributos, isto é, defina `scale.=TRUE`. 
Em seguida, use o comando `summary`, analise o resultado e os itens a seguir:

```{r atv2-code}

# Executando a redução de dimensionalidade com o prcomp
 # com normalização dos dados
pca_normalizado <- prcomp(dados_para_PCA, scale. = TRUE)

# Analisando as componentes com o comando summary
summary(pca_normalizado)

# Variância explicada por cada componente
var_explicada <- pca_normalizado$sdev^2 / sum(pca_normalizado$sdev^2)
var_acumulada <- cumsum(var_explicada)

n_80 <- which(var_acumulada >= 0.80)[1]
n_90 <- which(var_acumulada >= 0.90)[1]
n_95 <- which(var_acumulada >= 0.95)[1]
n_99 <- which(var_acumulada >= 0.99)[1]

n_80; n_90; n_95; n_99

```

## Análise

a) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `80%` do total? 
```{r}
which(var_acumulada >= 0.80)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 80% precisamos de 48 componentes.
<!-- Fim da resposta -->

b) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `90%` do total? 
```{r}
which(var_acumulada >= 0.90)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 90% precisamos de 112 componentes.
<!-- Fim da resposta -->

c) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `95%` do total? 
```{r}
which(var_acumulada >= 0.95)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 95% precisamos de 200 componentes.
<!-- Fim da resposta -->

d) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `99%` do total? 
```{r}
which(var_acumulada >= 0.99)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 99% precisamos de 401 componentes.
<!-- Fim da resposta -->

e) Quais as principais diferenças entre a aplicação do PCA nesse conjunto dados com e sem normalização? Qual opção parece ser mais adequada para esse conjunto de dados? Justifique sua resposta. 

**Resposta:** <!-- Escreva sua resposta abaixo -->
Quando aplicamos o PCA com normalização, percebemos que precisamos de mais componentes para explicar a mesma quantidade de informação do que sem normalização. Por exemplo, para chegar a 80% da variância, foram necessários 48 componentes com normalização, enquanto sem normalização bastavam 38. Isso acontece porque a normalização coloca todas as variáveis na mesma “escala”, dando a mesma importância para variáveis grandes e pequenas. Sem normalização, as variáveis com números maiores acabam dominando a análise.

Portanto, a normalização é útil quando queremos que todas as características tenham peso parecido na análise. No caso da base speech.csv, como as variáveis têm valores bem diferentes, usar PCA com normalização ajuda a considerar melhor todas as informações, mesmo que precisemos de mais componentes.

<!-- Fim da resposta -->


# Atividade 3 -- Visualização a partir da Redução (*2,0 pts*)

Nesta atividade, vamos aplicar diferentes métodos de redução de dimensionalidade e comparar as visualizações dos dados obtidos considerando apenas duas dimensões. Lembre de fixar uma semente antes de executar o T-SNE.

a) Aplique a redução de dimensionalidade com a técnica PCA e gere um gráfico de dispersão dos dados. Use a coluna `618` para classificar as amostras e definir uma coloração. 

```{r atv3a-code}

# Aplicando redução de dimensionalidade com a técnica PCA

# Gerando o gráfico de dispersão

```

b) Aplique a redução de dimensionalidade com a técnica UMAP e gere um gráfico de dispersão dos dados. Use a coluna `618` para classificar as amostras e definir uma coloração. 

```{r atv3b-code}

# Aplicando redução de dimensionalidade com a técnica UMAP

# Gerando o gráfico de dispersão


```

c) Aplique a redução de dimensionalidade com a técnica T-SNE e gere um gráfico de dispersão dos dados. Use a coluna `618` para classificar as amostras e definir uma coloração. 

```{r atv3c-code}

# Aplicando redução de dimensionalidade com a técnica T-SNE

# Gerando o gráfico de dispersão


```


## Análise

d) Qual técnica você acredita que apresentou a melhor projeção? Justifique.


**Resposta:** <!-- Escreva sua resposta abaixo -->


<!-- Fim da resposta -->

# Atividade 4 -- Agrupamento com o $K$*-means* (*2,0 pts*)

Com a melhor configuração encontrada na atividade 2, aplique a redução de dimensionalidade para que a variância acumulada seja pelo menos 80% do total. Na sequência, você deverá agrupar os dados com o algoritmo $K$*-means* e utilizará duas métricas básicas para a escolha do melhor $K$: a soma de distâncias intra-cluster e o coeficiente de silhueta. Se desejar, use também a função NbClust para ajudar nas análises.

**Implementações:**

a) Aplique a redução de dimensionalidade com a melhor configuração encontrada nas atividades 1 e 2 para que a variância acumulada seja pelo menos 80% do total.

```{r atv4a-code}
# Aplicando a redução de dimensionalidade no conjunto de dados original

```

b) *Gráfico \textsl{Elbow Curve}:* Construa um gráfico com a soma das distâncias intra-cluster para $K$ variando de $1$ a $30$.

```{r atv4b-code}
# Construindo um gráfico com as distâncias intra-cluster

```

c) *Gráfico da Silhueta:* Construa um gráfico com o valor da silhueta para $K$ variando de $1$ a $30$.

```{r atv4c-code}
# Construindo um gráfico com os valores da silhueta

```

d) *Escolha do $K$:* Avalie os gráficos gerados nos itens anteriores e escolha o melhor valor  de $K$ com base nas informações desses gráficos e na sua análise. Com o valor de $K$ definido, gere um gráfico de dispersão (atribuindo cores diferentes para cada grupo).

```{r atv4d-code}
# Aplicando o k-means com o k escolhido 

# Construindo um gráfico de dispersão

```


## Análise

e) Faça um breve resumo dos resultados, indicando a escolha do $K$ e a análise do gráfico de dispersão.


**Resposta:** <!-- Escreva sua resposta abaixo -->


<!-- Fim da resposta -->


# Atividade 5 -- Agrupamento com o *DBscan* (*2,0 pts*)

Com a melhor configuração encontrada na atividade 2, aplique a redução de dimensionalidade para que a variância acumulada seja pelo menos 80% do total. Depois, você deverá agrupar os dados com o algoritmo *DBscan*. Para isso será necessário experimentar com diferentes valores de *eps* e *minPts*. 

**Implementações:**

a) Aplique a redução de dimensionalidade com a melhor configuração encontrada nas atividades 1 e 2 para que a variância acumulada seja pelo menos 80% do total.

```{r atv5a-code}
# Aplicando a redução de dimensionalidade no conjunto de dados original

```

b) *Ajuste de Parâmetros:* Experimente com valores diferentes para os parâmetros *eps* e *minPts*. Verifique o impacto dos diferentes valores nos agrupamentos.

```{r atv5b-code}
# Experimento com valores de eps e minPts

# Experimento com valores de eps e minPts

# Experimento com valores de eps e minPts

```

c) *Determinando Ruídos:* Escolha o valor de *minPts* que obteve o melhor resultado no item anterior e use a função `kNNdistplot` do pacote `dbscan` para determinar o melhor valor de *eps* para esse valor de *minPts*. Lembre-se que o objetivo não é remover todos os ruídos. 

```{r atv5c-code}
# Encontrando o melhor eps com o kNNdistplot

```

d) *Visualizando os Grupos:* Após a escolha dos parâmetros *eps* e *minPts*, utilize o rótulo obtido para cada amostra, indicando o grupo ao qual ela pertence, para gerar um gráfico de dispersão (atribuindo cores diferentes para cada grupo). 

```{r atv5d-code}
# Aplicando o DBscan com os parâmetros escolhidos

# Construindo um gráfico de dispersão

```

## Análise

e) Faça um breve resumo dos resultados, indicando a escolha dos parâmetros *eps* e *minPts* e a análise do gráfico de dispersão.


**Resposta:** <!-- Escreva sua resposta abaixo -->


<!-- Fim da resposta -->





