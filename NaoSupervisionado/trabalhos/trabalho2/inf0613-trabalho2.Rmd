---
title:  INF0613 -- Aprendizado de Máquina Não Supervisionado
output: pdf_document
subtitle: Trabalho 2 - Redução de Dimensionalidade e Técnicas de Agrupamento
author: 
  - Vitor de Oliveira Fernandez Araujo
  - Vitor Sancho Cardoso
---

<!-- !!!!! Atenção !!!!! -->
<!-- Antes de fazer qualquer alteração neste arquivo, reabra-o com 
o encoding correto: 
File > Reopen with encoding > UTF-8
-->







```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, error = FALSE, message = FALSE, warning = FALSE, tidy = FALSE)
options(digits = 3)
```




O objetivo deste trabalho é exercitar o conhecimento de técnicas de redução de dimensionalidade e técnicas de agrupamento. Usaremos a base de dados `speech.csv`, que está disponível na página da disciplina no Moodle. A base contém amostras da pronúncia em inglês das letras do alfabeto.

# Atividade 0 -- Configurando o ambiente
Antes de começar a implementação do seu trabalho configure o _workspace_ e importe todos os pacotes e execute o preprocessamento da base:

```{r atv0-code}
# Adicione os demais pacotes usados neste trabalho:
library(ggplot2)
library(umap)
library(Rtsne)
library(cluster)
set.seed(123)

# Configure ambiente de trabalho na mesma pasta 
# onde colocou a base de dados:
# setwd("")

# Pré-processamento da base de dados
# Lendo a base de dados
speech <- read.csv("speech.csv", header = TRUE)

# Convertendo a coluna 618 em characteres 
speech$LETRA <- LETTERS[speech$LETRA]

```



# Atividade 1 -- Análise de Componentes Principais (*2,0 pts*)

Durante a redução de dimensionalidade, espera-se que o poder de representação do conjunto de dados seja mantido, para isso é preciso realizar uma análise da variância mantida em cada componente principal obtido. Use função  `prcomp`, que foi vista em aula, para criar os autovetores e autovalores da base de dados. Não use a normalização dos atributos, isto é, defina  `scale.=FALSE`. 
Em seguida, use o comando `summary`, analise o resultado e os itens a seguir:


<!-- Use o comando: options(max.print=2000) para visualizar o resultado 
do comando summary e fazer suas análises. Não é necessário que toda informação 
do comando summary apareça no PDF a ser submetido. Desse modo, repita o comando 
com um valor mais baixo antes de gerar a versão final do PDF. -->

```{r atv1-code}

# Executando a redução de dimensionalidade com o prcomp
dados_para_PCA <- speech[, 1:617]
pca <- prcomp(dados_para_PCA, scale. = FALSE)

# Analisando as componentes com o comando summary
options(max.print=50)
summary(pca)

# Analise de variância explicada
var_exp <- (pca$sdev^2) / sum(pca$sdev^2)
cum_var_exp <- cumsum(var_exp)
head(cum_var_exp, 20)
```

## Análise

a) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `80%` do total? 
```{r}
which(cum_var_exp >= 0.80)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 80% precisamos de 38 componentes.
<!-- Fim da resposta -->

b) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `90%` do total? 
```{r}
which(cum_var_exp >= 0.90)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 90% precisamos de 91 componentes.
<!-- Fim da resposta -->

c) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `95%` do total? 
```{r}
which(cum_var_exp >= 0.95)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 95% precisamos de 170 componentes.
<!-- Fim da resposta -->

d) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `99%` do total? 
```{r}
which(cum_var_exp >= 0.99)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 99% precisamos de 382 componentes.
<!-- Fim da resposta -->

e) Faça um breve resumo dos resultados dos itens *a)-d)* destacando o impacto da redução de dimensionalidade. 
**Resposta:** <!-- Escreva sua resposta abaixo -->
A análise dos componentes principais mostra que a variabilidade dos dados pode ser representada de forma eficiente por um subconjunto dos componentes. Observa-se que os primeiros 38 componentes capturam aproximadamente 80% da variância total, indicando que grande parte da informação está concentrada nesses poucos componentes iniciais. Ao aumentar o nível de variância desejada, a quantidade de componentes necessários cresce: 91 componentes explicam 90% da variância, 170 componentes explicam 95%, e para atingir 99% da variância, é necessário manter 382 componentes.

Esses resultados evidenciam que, embora seja possível reduzir a dimensionalidade do conjunto de dados, existe um trade-off entre simplificação e preservação da informação. A redução de dimensionalidade ajuda a eliminar redundâncias e ruídos, facilita visualização, diminui custos computacionais e pode melhorar o desempenho de algoritmos de aprendizado de máquina. Os resultados mostram que a estrutura dos dados que temos para analisar é complexa, sendo necessário utilizar muitos componentes para conservar a maior parte da informação (99% da variância). Isto nos indica que os pontos em alta dimensão representados no conjunto não têm relações lineares de correlação muito fortes em nenhuma direção em particular
<!-- Fim da resposta -->

# Atividade 2 -- Análise de Componentes Principais e Normalização (*2,0 pts*)

A normalização de dados em alguns casos, pode trazer benefícios. Nesta questão, iremos analisar o impacto dessa prática na redução da dimensionalidade da base de dados `speech.csv`. Use função  `prcomp` para criar os autovetores e autovalores da base de dados usando a normalização dos atributos, isto é, defina `scale.=TRUE`. 
Em seguida, use o comando `summary`, analise o resultado e os itens a seguir:

```{r atv2-code}

# Executando a redução de dimensionalidade com o prcomp
 # com normalização dos dados
pca_normalizado <- prcomp(dados_para_PCA, scale. = TRUE)

# Analisando as componentes com o comando summary
options(max.print=50)
summary(pca_normalizado)

# Variância explicada por cada componente
var_explicada <- pca_normalizado$sdev^2 / sum(pca_normalizado$sdev^2)
var_acumulada <- cumsum(var_explicada)

n_80 <- which(var_acumulada >= 0.80)[1]
n_90 <- which(var_acumulada >= 0.90)[1]
n_95 <- which(var_acumulada >= 0.95)[1]
n_99 <- which(var_acumulada >= 0.99)[1]

n_80; n_90; n_95; n_99

```

## Análise

a) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `80%` do total? 
```{r}
which(var_acumulada >= 0.80)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 80% precisamos de 48 componentes.
<!-- Fim da resposta -->

b) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `90%` do total? 
```{r}
which(var_acumulada >= 0.90)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 90% precisamos de 112 componentes.
<!-- Fim da resposta -->

c) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `95%` do total? 
```{r}
which(var_acumulada >= 0.95)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 95% precisamos de 200 componentes.
<!-- Fim da resposta -->

d) Qual o menor número de componentes, tal que a variância acumulada seja pelo menos `99%` do total? 
```{r}
which(var_acumulada >= 0.99)[1]
```
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para carregar uma variância acumulada de ao menos 99% precisamos de 401 componentes.
<!-- Fim da resposta -->

e) Quais as principais diferenças entre a aplicação do PCA nesse conjunto dados com e sem normalização? Qual opção parece ser mais adequada para esse conjunto de dados? Justifique sua resposta. 

**Resposta:** <!-- Escreva sua resposta abaixo -->
Quando aplicamos o PCA com normalização, percebemos que precisamos de mais componentes para explicar a mesma quantidade de informação do que sem normalização. Por exemplo, para chegar a 80% da variância, foram necessários 48 componentes com normalização, enquanto sem normalização bastavam 38. Isso acontece porque a normalização coloca todas as variáveis na mesma “escala”, dando a mesma importância para variáveis grandes e pequenas. Sem normalização, as variáveis com números maiores acabam dominando a análise.

No caso deste dataset, a normalização piorou a performance do PCA, pois todos os dados já parecem estar numa mesma "unidade" (assumindo que os dados representam algo como intensidades de som em diferentes frequências). Por este motivo, ao aplicar a normalização, acabamos perdendo um pouco de variância no conjunto como um todo, tornando a representação por PCA mais difícil.

<!-- Fim da resposta -->


# Atividade 3 -- Visualização a partir da Redução (*2,0 pts*)

Nesta atividade, vamos aplicar diferentes métodos de redução de dimensionalidade e comparar as visualizações dos dados obtidos considerando apenas duas dimensões. Lembre de fixar uma semente antes de executar o T-SNE.

a) Aplique a redução de dimensionalidade com a técnica PCA e gere um gráfico de dispersão dos dados. Use a coluna `618` para classificar as amostras e definir uma coloração. 

```{r atv3a-code}

# Aplicando redução de dimensionalidade com a técnica PCA
pca_res <- prcomp(speech[,1:617], scale.=FALSE)

# Pegando as duas primeiras componentes
pca_2d <- data.frame(PC1 = pca_res$x[,1],
                     PC2 = pca_res$x[,2],
                     LETRA = speech$LETRA)

# Gerando gráfico de dispersão

ggplot(pca_2d, aes(x=PC1, y=PC2, label=LETRA, color=LETRA)) +
  geom_text(size=2) +
  labs(title="PCA - Redução para 2 dimensões",
       x="PC1",
       y="PC2") +
  theme_minimal() +
  theme(legend.position="right")
```

b) Aplique a redução de dimensionalidade com a técnica UMAP e gere um gráfico de dispersão dos dados. Use a coluna `618` para classificar as amostras e definir uma coloração. 

```{r atv3b-code}

# Aplicando redução de dimensionalidade com a técnica UMAP
set.seed(123)
umap_res <- umap(speech[,1:617])

umap_2d <- data.frame(
  UMAP1 = umap_res$layout[,1],
  UMAP2 = umap_res$layout[,2],
  LETRA = speech$LETRA
)

# Gerando o gráfico de dispersão
ggplot(umap_2d, aes(x=UMAP1, y=UMAP2, label=LETRA, color=LETRA)) +
  geom_text(size=3) +
  labs(title="UMAP - Redução para 2 dimensões",
       x="UMAP1", y="UMAP2") +
  theme_minimal() +
  theme(legend.position="right")

```

c) Aplique a redução de dimensionalidade com a técnica T-SNE e gere um gráfico de dispersão dos dados. Use a coluna `618` para classificar as amostras e definir uma coloração. 

```{r atv3c-code}

# Aplicando redução de dimensionalidade com a técnica T-SNE
set.seed(123)
tsne_res <- Rtsne(speech[,1:617], dims=2, perplexity=100, verbose=TRUE)

tsne_2d <- data.frame(
  TSNE1 = tsne_res$Y[,1],
  TSNE2 = tsne_res$Y[,2],
  LETRA = speech$LETRA
)

# Gerando gráfico de dispersão
ggplot(tsne_2d, aes(x=TSNE1, y=TSNE2, color=LETRA, label=LETRA)) +
  geom_text(size=3) +
  labs(title="t-SNE - Redução para 2 dimensões",
       x="TSNE1", y="TSNE2") +
  theme_minimal() +
  theme(legend.position="right")

```


## Análise

d) Qual técnica você acredita que apresentou a melhor projeção? Justifique.
**Resposta:** <!-- Escreva sua resposta abaixo -->
O PCA mostra uma grande sobreposição entre os grupos de letras. Muitos pontos de diferentes letras aparecem misturados, e é difícil identificar aglomerados distintos. Isso acontece porque o PCA foca apenas na variância dos dados, sem tentar preservar a estrutura local ou a similaridade entre os pontos. Sua medida de proximidade é exclusivamente linear, então para o caso de dados com relações complexas, como observamos aqui, sua eficácia de separação se torna baixa. 
O UMAP mostra os grupos muito mais definidos e compactos. Cada letra forma um aglomerado mais definido, com menor sobreposição entre os grupos. A separação clara dos grupos permite perceber mais claramente as relações entre as letras, por exemplo, podemos ver uma proximidade entre as letras M e N, e F e S, cujas pronúncias em inglês realmente são bem parecidas. Além disso, temos um outro grande grupo formado majoritariamente por V, G, J e K, em que se sugere que estas letras podem ter pronúncias parecidas, a depender do sotaque de quem fala, pois estão presentes em mais de um cluster e, no maior cluster elas cobrem uma grande área, mostrando grande variação em sua pronúncia.
No t-SNE, observamos uma quantidade maior de grupos, que conservam uma boa definição de cada letra, mas estão mais próximos entre si. Porém, alguns grupos ainda aparecem misturados, principalmente no centro, e os pontos estão espalhados de forma irregular, o que pode dificultar identificar claramente todos os grupos. Testamos com os valores de perplexidade 10, 30, 50 e 100. Acreditamos ter o melhor resultado com o valor 100, que nos permitiu enxergar estruturas parecidas com o UMAP, apesar de ainda assim ter um menor afastamento entre grupos. 
Levando isso em consideração, podemos concluir que o UMAP parece ter apresentado a melhor projeção. Ele organiza os pontos de forma mais clara e separa os grupos de letras de maneira bem definida, tornando fácil visualizar quais pontos pertencem a cada letra. O t-SNE funciona bem, mas constroi uma estrutura global mais compacta, bagunçando a visualização. O PCA não consegue competir com ambos, pois se restringe a relações lineares e, por isso, mistura os pontos completamente com apenas 2 componentes principais. Em termos simples, UMAP “arrumou a bagunça” melhor que as outras duas técnicas.
<!-- Fim da resposta -->

# Atividade 4 -- Agrupamento com o $K$*-means* (*2,0 pts*)

Com a melhor configuração encontrada na atividade 2, aplique a redução de dimensionalidade para que a variância acumulada seja pelo menos 80% do total. Na sequência, você deverá agrupar os dados com o algoritmo $K$*-means* e utilizará duas métricas básicas para a escolha do melhor $K$: a soma de distâncias intra-cluster e o coeficiente de silhueta. Se desejar, use também a função NbClust para ajudar nas análises.

**Implementações:**

a) Aplique a redução de dimensionalidade com a melhor configuração encontrada nas atividades 1 e 2 para que a variância acumulada seja pelo menos 80% do total.

```{r atv4a-code}
# Aplicando a redução de dimensionalidade no conjunto de dados original
dados_para_PCA <- speech[, 1:617]

# Aplica PCA sem normalização, pois foi o que apresentou melhor resultado
pca <- prcomp(dados_para_PCA, scale. = FALSE)

var_acum <- cumsum(pca$sdev^2) / sum(pca$sdev^2)
num_comp <- which(var_acum >= 0.80)[1]
num_comp

# Reduzindo o conjunto de features
dados_reduzidos <- pca$x[, 1:num_comp]

```

b) *Gráfico \textsl{Elbow Curve}:* Construa um gráfico com a soma das distâncias intra-cluster para $K$ variando de $1$ a $30$.

```{r atv4b-code}
# Construindo um gráfico com as distâncias intra-cluster
wss <- sapply(1:30, function(k){
  kmeans(dados_reduzidos, centers=k, nstart=10)$tot.withinss
})

plot(1:30, wss, type="b", pch=19, frame=FALSE,
     xlab="Número de clusters K",
     ylab="Soma de quadrados intra-cluster (WSS)",
     main="Elbow Curve")
```

c) *Gráfico da Silhueta:* Construa um gráfico com o valor da silhueta para $K$ variando de $1$ a $30$.

```{r atv4c-code}
# Construindo um gráfico com os valores da silhueta
sil <- sapply(2:30, function(k){
  ss <- silhouette(km$cluster, dist(dados_reduzidos))
  mean(ss[, 3])  # média da largura da silhueta
})

plot(2:30, sil, type="b", pch=19, frame=FALSE,
     xlab="Número de clusters K",
     ylab="Coeficiente médio de silhueta",
     main="Silhouette")
```

d) *Escolha do $K$:* Avalie os gráficos gerados nos itens anteriores e escolha o melhor valor  de $K$ com base nas informações desses gráficos e na sua análise. Com o valor de $K$ definido, gere um gráfico de dispersão (atribuindo cores diferentes para cada grupo).

```{r atv4d-code}
# Aplicando o k-means com o k escolhido 
k = 15
km_final <- kmeans(dados_reduzidos, centers=k, nstart=25)

# Prepara dados para plotar (apenas 2 PCs para visualização)
dados_plot <- data.frame(PC1 = dados_reduzidos[,1],
                         PC2 = dados_reduzidos[,2],
                         cluster = factor(km_final$cluster))

# Construindo um gráfico de dispersão
ggplot(dados_plot, aes(x=PC1, y=PC2, color=cluster)) +
  geom_point(alpha=0.7) +
  labs(title=paste("K-means com K =", k),
       x="PC1", y="PC2") +
  theme_minimal()

```


## Análise

e) Faça um breve resumo dos resultados, indicando a escolha do $K$ e a análise do gráfico de dispersão.


**Resposta:** <!-- Escreva sua resposta abaixo -->


<!-- Fim da resposta -->


# Atividade 5 -- Agrupamento com o *DBscan* (*2,0 pts*)

Com a melhor configuração encontrada na atividade 2, aplique a redução de dimensionalidade para que a variância acumulada seja pelo menos 80% do total. Depois, você deverá agrupar os dados com o algoritmo *DBscan*. Para isso será necessário experimentar com diferentes valores de *eps* e *minPts*. 

**Implementações:**

a) Aplique a redução de dimensionalidade com a melhor configuração encontrada nas atividades 1 e 2 para que a variância acumulada seja pelo menos 80% do total.

```{r atv5a-code}
# Aplicando a redução de dimensionalidade no conjunto de dados original

```

b) *Ajuste de Parâmetros:* Experimente com valores diferentes para os parâmetros *eps* e *minPts*. Verifique o impacto dos diferentes valores nos agrupamentos.

```{r atv5b-code}
# Experimento com valores de eps e minPts

# Experimento com valores de eps e minPts

# Experimento com valores de eps e minPts

```

c) *Determinando Ruídos:* Escolha o valor de *minPts* que obteve o melhor resultado no item anterior e use a função `kNNdistplot` do pacote `dbscan` para determinar o melhor valor de *eps* para esse valor de *minPts*. Lembre-se que o objetivo não é remover todos os ruídos. 

```{r atv5c-code}
# Encontrando o melhor eps com o kNNdistplot

```

d) *Visualizando os Grupos:* Após a escolha dos parâmetros *eps* e *minPts*, utilize o rótulo obtido para cada amostra, indicando o grupo ao qual ela pertence, para gerar um gráfico de dispersão (atribuindo cores diferentes para cada grupo). 

```{r atv5d-code}
# Aplicando o DBscan com os parâmetros escolhidos

# Construindo um gráfico de dispersão

```

## Análise

e) Faça um breve resumo dos resultados, indicando a escolha dos parâmetros *eps* e *minPts* e a análise do gráfico de dispersão.


**Resposta:** <!-- Escreva sua resposta abaixo -->


<!-- Fim da resposta -->





