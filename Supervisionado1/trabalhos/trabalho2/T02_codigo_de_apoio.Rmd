---
title: INF0615 -- Aprendizado de Máquina Supervisionado I
output: pdf_document
subtitle: Trabalho 2 - Risco de Hipertensão
author: 
  - Vitor de Oliveira Fernandez Araujo
  - Vitor Sancho Cardoso
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(
  echo    = TRUE,   # Exibir código nos chunks
  error   = FALSE,  # Ocultar mensagens de erro
  message = FALSE,  # Ocultar mensagens informativas
  warning = FALSE,  # Ocultar avisos
  tidy    = FALSE   # Não reformatar automaticamente o código
)

options(digits = 4) # Definição do número de casas decimais padrão
```

Neste segundo trabalho, exploraremos a predição de risco de hipertensão utilizando regressão logística, árvore de decisão e floresta aleatória.

O conjunto de dados está disponível na página da disciplina no Moodle (arquivo `T02_SPLIT_set.csv`).

# Função de apoio

## getHypothesis

Essa função auxilia na escrita dos modelos polinomiais.

Parâmetros:

\- `X_features_names`: conjunto de *features* a serem consideradas na criação do modelo polinomial;

\- `target`: nome da coluna variável alvo do conjunto de dados;

\- `degree`: indica até qual grau polinomial as *features* serão elevadas.

```{r}
# A funcao abaixo auxilia na escrita dos modelos polinomiais. 
# Parametros:
# "real_feature_names": conjunto de features continuas que sera considerado na
#                        criacao do modelo polinomial.
#
# "categorical_feature_names": conjunto de features categoricas que sera 
#                               considerado na  criacao do modelo polinomial. Se
#                                voces desejarem um modelo sem variaveis categoricas
#                               basta nao passar nenhum valor para este parametro
#                               na chamada da funcao
# "target": nome da variável target de interesse
# "degree": numero inteiro que indica ate qual grau polinomial as features continuas
#           em "real_feature_names" serao elevadas. 
#
# A funcao retorna a hipotese ja definida para realizar o treinamento do modelo. 
# Uma ilustracao de uma funcao similar aparece no Ex02.R na linha 490

getHypothesis <- function(real_feature_names, categorical_feature_names=F, target="Has_Hypertension", degree=3){
    
    hypothesis_string <- paste("hypothesis <- formula(", target, " ~ ")
    for(d in 1:degree){
        for(i in 1:length(real_feature_names)){
            hypothesis_string <- paste(hypothesis_string, 
                                       "I(", real_feature_names[i], "^", d, ") + ",
                                       sep = "")
        }
    }
    
    if(typeof(categorical_feature_names) != "logical"){
        for(i in 1:length(categorical_feature_names)){
            hypothesis_string <- paste(hypothesis_string, 
                                       categorical_feature_names[i], " + ",
                                       sep = "")
        } 
    }
    
    
    hypothesis_string <- substr(hypothesis_string, 1, nchar(hypothesis_string)-3)
    hypothesis_string <- paste(hypothesis_string, ")")
    hypothesis <- eval(parse(text=hypothesis_string))
    return(hypothesis)
}
```

## getLoss

Função que calcula a *Cross Entropy Loss*. Ela é útil para mostrar em gráficos de curva de viés e variância o erro de classificadores.

Parâmetros:

\- `y_true`: vetor das classes verdadeiras (*ground truth*);

\- `y_pred`: vetor das classes preditas pelo classificador.

```{r}
getLoss <- function(y_true, y_pred){
  y_pred_n <- y_pred[y_true == 0]
  y_pred_p <- y_pred[y_true == 1]
  
  countN <- length(y_pred_n)
  countP <- length(y_pred_p)
  
  eps <- 1e-9  # Constante pequena para estabilidade numérica

  totalLossN <- sum(-log2(1 - y_pred_n + eps))
  
  totalLossP <- sum(-log2(y_pred_p + eps))
  
  avgLossN <- totalLossN / countN
  avgLossP <- totalLossP / countP

  avgLossTotal <- (avgLossN + avgLossP) / 2

  return(c(LossN = avgLossN, LossP = avgLossP, AverageLoss = avgLossTotal))
}
```

## getRelativeConfusionMatrix

Função que calcula a matriz de confusão relativa.

Paramêtro:

\- `cm`: matriz de confusão absoluta.

```{r}
getRelativeConfusionMatrix <- function(cm){
    cm_absolute = t(cm$table)
    
    cm_relative = cm_absolute
    
    cm_relative[1,1] = cm_absolute[1,1]/sum(cm_absolute[1,])
    cm_relative[1,2] = cm_absolute[1,2]/sum(cm_absolute[1,])
    cm_relative[2,1] = cm_absolute[2,1]/sum(cm_absolute[2,])
    cm_relative[2,2] = cm_absolute[2,2]/sum(cm_absolute[2,])
    
    return(cm_relative)  
}
```

## getMetrics

Função que calcula diversas métricas de avaliação a partir de uma matriz de confusão.

Parâmtro:

\- `cm`: matriz de confusão (cm\$table).

```{r}
calculateMetrics <- function(cm) {
  # Extrai TN, FP, FN, TP da matriz de confusão
  cm <- t(cm)
  
  # Assumindo a estrutura:
  #           Prediction
  # Reference   0     1
  #         0   TN    FP
  #         1   FN    TP
  
  TN <- cm[1, 1]
  FP <- cm[1, 2]
  FN <- cm[2, 1]  
  TP <- cm[2, 2]  
  
  precision <- if (TP + FP == 0) 0 else TP / (TP + FP)
  recall    <- if (TP + FN == 0) 0 else TP / (TP + FN)
  f1        <- if (precision + recall == 0) 0 else (2 * precision * recall) / (precision + recall)
  bal_acc   <- ( (TN / (TN + FP)) + (TP / (TP + FN)) ) / 2
  
  return(c(Precision = precision, Recall = recall, F1 = f1, BalAcc = bal_acc))
}

## Exemplo de uso
# cm <- confusionMatrix(data = as.factor(valPred), reference = as.factor(ValSet$target), positive='1')
# metrics <- calculateMetrics(cm$table)
```

# Tarefa 0 -- Configurando o ambiente

Carregue as bibliotecas necessárias e defina uma semente aleatória para garantir a reprodutibilidade dos experimentos.

```{r atv0-code}
# Adicione as bibliotecas necessárias
#library(...)
# install.packages("vcd")
# install.packages("glmnet")
library(ggplot2)
library(vcd)
library(glmnet)
library(caret)
library(corrplot)
library(rpart)        # Árvore de decisão
library(rpart.plot)   # Visualização de árvores de decisão
library(randomForest) # Random Forest
library(reshape2) 

set.seed(42)
```

# Tarefa 1 -- Inspeção de Dados

Carreguem e inspecionem o conjunto de dados. Além disso, preparem os dados corretamente para serem utilizados nos classificadores, preocupando-se com o balanceamento de classes (se necessário) e normalização (se necessário).

```{r atv1-code}
# Carregue o conjunto de dados
dados_treino <- read.csv("T02_train_set.csv")
dados_validacao <- read.csv("T02_valid_set.csv")

# Sumário da base
paste("O dataframe de TREINO possui",nrow(dados_treino),"registros",sep=" ")
paste("summary TREINO:")
summary(dados_treino)

if (any(is.na(dados_treino))) {
  cat("Aviso: Existem valores ausentes no conjunto de validação.\n")
} else {
  cat("Nenhum valor ausente encontrado no conjunto de validação.\n")
}

paste("O dataframe de VALIDAÇÃO possui",nrow(dados_validacao),"registros",sep=" ")
paste("summary VALIDAÇÃO:")
summary(dados_validacao)

if (any(is.na(dados_validacao))) {
  cat("Aviso: Existem valores ausentes no conjunto de validação.\n")
} else {
  cat("Nenhum valor ausente encontrado no conjunto de validação.\n")
}

# Avaliando dados presentes no data set
# class(dados_treino$Consumo.Sal)
# class(dados_treino$Nivel.de.Estresse)
# class(dados_treino$Historico.Pressao.Arterial)
# class(dados_treino$IMC)
# class(dados_treino$Historico.Familiar)
# class(dados_treino$Nivel.Exercicio)
# class(dados_treino$Fumante)
# class(dados_treino$Risco.Hipertensao)
# 
 # head(dados_treino)
# 
# head(dados_validacao)
# 
# class(dados_validacao$Consumo.Sal)
# class(dados_validacao$Nivel.de.Estresse)
# class(dados_validacao$Historico.Pressao.Arterial)
# class(dados_validacao$IMC)
# class(dados_validacao$Historico.Familiar)
# class(dados_validacao$Nivel.Exercicio)
# class(dados_validacao$Fumante)
# class(dados_validacao$Risco.Hipertensao)
# 
# head(dados_validacao)


# Normalização
# Aplicando normalização min-max
treino_normalizado <- dados_treino
validacao_normalizado <- dados_validacao
cols <- c(1, 2, 4, 5)  # índices das colunas numericas que vamos normalizar

min_features <- apply(treino_normalizado[, cols], 2, min)
max_features <- apply(treino_normalizado[, cols], 2, max)
diff         <- max_features - min_features

treino_normalizado[, cols] <- sweep(treino_normalizado[, cols], 2, min_features, "-")
treino_normalizado[, cols] <- sweep(treino_normalizado[, cols], 2, diff, "/")

validacao_normalizado[, cols] <- sweep(validacao_normalizado[, cols], 2, min_features, "-")
validacao_normalizado[, cols] <- sweep(validacao_normalizado[, cols], 2, diff, "/")

summary(validacao_normalizado)

```


Análise da distribuição dos dados de treino

```{r}
# Histograma do Consumo de Sal normalizado
ggplot(treino_normalizado, aes(x = Consumo.Sal)) +
  geom_histogram(fill = "orange", color = "black") +
  theme_minimal() +
  labs(title = "Distribuição do Consumo de Sal",
       x = "Consumo de Sal normalizado", y = "Densidade")

# Histograma da Duracao de Sono normalizad
ggplot(treino_normalizado, aes(x = Duracao.Sono)) +
  geom_histogram(fill = "orange", color = "black") +
  theme_minimal() +
  labs(title = "Distribuição do Duracao Sono", x = "Duracao de Sono normalizada", y = "Densidade")

# Histograma do duração do IMC normalizado
ggplot(treino_normalizado, aes(x = IMC)) +
  geom_histogram(fill = "orange", color = "black") +
  theme_minimal() +
  labs(title = "Distribuição do IMC",
       x = "IMC normalizado", y = "Densidade")

# Contagem de ocorrencias por Nivel de Estresse
ggplot(treino_normalizado, aes(x = Nivel.de.Estresse, fill = Nivel.de.Estresse)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribuição por Nivel de Estresse", x = "Nivel de Estresse", y = "Contagem")

# Contagem de ocorrencias por Historico de Pressao Arterial
ggplot(treino_normalizado, aes(x = Historico.Pressao.Arterial, fill = Historico.Pressao.Arterial)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribuição por Historico de Pressao Arterial", x = "Historico de Pressao Arterial", y = "Contagem")

# Contagem de ocorrencias por Historico Familiar
ggplot(treino_normalizado, aes(x = Historico.Familiar, fill = Historico.Familiar)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribuição por Historico Familiar", x = "Historico Familiar", y = "Contagem")

# Contagem de ocorrencias por Nivel de Exercicio
ggplot(treino_normalizado, aes(x = Nivel.Exercicio, fill = Nivel.Exercicio)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribuição por Nivel de Exercicio", x = "Nivel de Exercicio", y = "Contagem")

# Contagem de ocorrencias por Fumante
ggplot(treino_normalizado, aes(x = Fumante, fill = Fumante)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribuição por Fumante", x = "Fumante", y = "Contagem")

ggplot(treino_normalizado, aes(x = Risco.Hipertensao, fill = Risco.Hipertensao)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribuição por Risco.Hipertensao", x = "Risco.Hipertensao", y = "Contagem")
```

Análise da distribuição dos dados de validação

```{r}
# Histograma do Consumo de Sal normalizado
ggplot(validacao_normalizado, aes(x = Consumo.Sal)) +
  geom_histogram(fill = "orange", color = "black") +
  theme_minimal() +
  labs(title = "Distribuição do Consumo de Sal",
       x = "Consumo de Sal normalizado", y = "Densidade")

# Histograma da Duracao de Sono normalizad
ggplot(validacao_normalizado, aes(x = Duracao.Sono)) +
  geom_histogram(fill = "orange", color = "black") +
  theme_minimal() +
  labs(title = "Distribuição do Duracao Sono", x = "Duracao de Sono normalizada", y = "Densidade")

# Histograma do duração do IMC normalizado
ggplot(validacao_normalizado, aes(x = IMC)) +
  geom_histogram(fill = "orange", color = "black") +
  theme_minimal() +
  labs(title = "Distribuição do IMC",
       x = "IMC normalizado", y = "Densidade")

# Contagem de ocorrencias por Nivel de Estresse
ggplot(validacao_normalizado, aes(x = Nivel.de.Estresse, fill = Nivel.de.Estresse)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribuição por Nivel de Estresse", x = "Nivel de Estresse", y = "Contagem")

# Contagem de ocorrencias por Historico de Pressao Arterial
ggplot(validacao_normalizado, aes(x = Historico.Pressao.Arterial, fill = Historico.Pressao.Arterial)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribuição por Historico de Pressao Arterial", x = "Historico de Pressao Arterial", y = "Contagem")

# Contagem de ocorrencias por Historico Familiar
ggplot(validacao_normalizado, aes(x = Historico.Familiar, fill = Historico.Familiar)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribuição por Historico Familiar", x = "Historico Familiar", y = "Contagem")

# Contagem de ocorrencias por Nivel de Exercicio
ggplot(validacao_normalizado, aes(x = Nivel.Exercicio, fill = Nivel.Exercicio)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribuição por Nivel de Exercicio", x = "Nivel de Exercicio", y = "Contagem")

# Contagem de ocorrencias por Fumante
ggplot(validacao_normalizado, aes(x = Fumante, fill = Fumante)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribuição por Fumante", x = "Fumante", y = "Contagem")

ggplot(validacao_normalizado, aes(x = Risco.Hipertensao, fill = Risco.Hipertensao)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribuição por Risco.Hipertensao", x = "Risco.Hipertensao", y = "Contagem")

```

One hot encoding de features categóricas:

```{r}
# One Hot Encoding
paste("One hot encoding para conjunto de TREINO")
dados_para_encoding_treino <- treino_normalizado

dados_para_encoding_treino$Historico.Pressao.Arterial <- as.factor(dados_para_encoding_treino$Historico.Pressao.Arterial)
one_hot <- model.matrix(~ Historico.Pressao.Arterial - 1, data = dados_para_encoding_treino)
dados_para_encoding_treino <- cbind(dados_para_encoding_treino, one_hot)
dados_para_encoding_treino$Historico.Pressao.Arterial <- NULL
colnames(dados_para_encoding_treino) <- gsub("-", "_", colnames(dados_para_encoding_treino))

dados_para_encoding_treino$Nivel.Exercicio <- as.factor(dados_para_encoding_treino$Nivel.Exercicio)
one_hot <- model.matrix(~ Nivel.Exercicio - 1, data = dados_para_encoding_treino)
dados_para_encoding_treino <- cbind(dados_para_encoding_treino, one_hot)
dados_para_encoding_treino$Nivel.Exercicio <- NULL
colnames(dados_para_encoding_treino) <- gsub("-", "_", colnames(dados_para_encoding_treino))

paste("One hot encoding para conjunto de VALIDAÇÃO")

dados_para_encoding_validacao <- validacao_normalizado

dados_para_encoding_validacao$Historico.Pressao.Arterial <- as.factor(dados_para_encoding_validacao$Historico.Pressao.Arterial)
one_hot <- model.matrix(~ Historico.Pressao.Arterial - 1, data = dados_para_encoding_validacao)
dados_para_encoding_validacao <- cbind(dados_para_encoding_validacao, one_hot)
dados_para_encoding_validacao$Historico.Pressao.Arterial <- NULL
colnames(dados_para_encoding_validacao) <- gsub("-", "_", colnames(dados_para_encoding_validacao))

dados_para_encoding_validacao$Nivel.Exercicio <- as.factor(dados_para_encoding_validacao$Nivel.Exercicio)
one_hot <- model.matrix(~ Nivel.Exercicio - 1, data = dados_para_encoding_validacao)
dados_para_encoding_validacao <- cbind(dados_para_encoding_validacao, one_hot)
dados_para_encoding_validacao$Nivel.Exercicio <- NULL
colnames(dados_para_encoding_validacao) <- gsub("-", "_", colnames(dados_para_encoding_validacao))
```




1.1 – Quantas amostras existem no conjunto de dados?
**Resposta:** <!-- Escreva sua resposta abaixo -->
No counto de treino temos 1349 amostras, já no validação temos 338.
<!-- Fim da resposta -->

1.2 – Existem amostras com valores ausentes? Como vocês pretendem tratar esses casos?
**Resposta:** <!-- Escreva sua resposta abaixo -->
Não observamos amostras com valores ausentes ou inválidos, tanto no conjunto de dados de treino quando no de validação.
<!-- Fim da resposta -->

1.3 – Esse conjunto de dados é balanceado? Caso não seja, como pretendem tratá-lo e por que usar essa técnica específica?
**Resposta:** <!-- Escreva sua resposta abaixo -->
Acima fizemos o plot da distribuição de dados - já normalizada para o caso de features com valores contínuos -, para cada uma das 8 faetures presentes no conjunto de dados que caracterizam as amostras: Consumo de sal, duração do sono, IMC, nível de estresse, histórico de pressão arterial, histórico familiar, nível de exercício e quantidade de fumantes. Também o fizemos para a feature alvo no modelo de predição a ser desenvolvido: Risco de hipertensão. Sobre a normalização, aplicamos a técnica de min-max.
As festures consumo de sal, duração do sono e IMC apresetam uma distribuição normal dos dados, já a feature de nível de estresse tem uma distribuição uniforme. Essas features possuem dados contínuos e a distribuição de seus valores é bem comportada, seguindo distribuição normal ou contínua, o que não afeta a qualidade do treinamento do modelo. 
O dataset também possui features categóricas, que apresentam valores discretos. Para elas montamos gráficos com a quantidade de itens por cada valor possíveis para a feature. A análise da feature de histórico familiar indica que a quantidade de pessoas com valor positivo e negativo é quase igual. A de pressão arterial possui três possíveis valores: Hipertensão, Normal e Pre-hipertensão. Isso não foi observado nas outras duas features categóricas do data set, o nível de exercício e fumante. Na primeira é possível notar uma grande diferença individuos com diferentes níveis de exercídios, temos mais de 600 indivíduos com baixo nível de exercício, por volta de 270 com alto (menos da metado da quantidade de alto) e algo am torno de 430 com moderado.Essa diferença também é notória na feature de fumantes. 
Para a nossa feature de alvo - Risco de Hipertensão - não é possível notar um grau preocupante de desbalanceamento. Por isso não é necessário aplicar nenhuma técnica para equilibrar a quantidade de indivíduos com ou sem Risco de Hipertensão.
Essa análise também foi realizada para o conjunto de dados de validação, que apresentou um perfil de distribuição, para cada uma das features, muito similiar ao de treino.
<!-- Fim da resposta -->

1.4 – Há alguma característica dos dados que pode impactar o desempenho do modelo?
A distribuição não igualitária das features nível de exercício e fumante chamou nossa atenção, e pode afetar a qualidade do modelo. 
**Resposta:** <!-- Escreva sua resposta abaixo -->

<!-- Fim da resposta -->

# Tarefa 2 -- Regressão Logística

Treinem um modelo *baseline* de regressão logística utilizando **todas as features disponíveis**.

```{r atv2-code}
train_data <- dados_para_encoding_treino
valid_data <- dados_para_encoding_validacao
# nome da coluna que você quer mandar para o fim
coluna <- "Risco.Hipertensao"

# reordenando colunas, de forma que feature de interesse seja a ultima
train_data <- train_data[, c(setdiff(names(train_data), coluna), coluna)]
feature_names <- colnames(train_data)[1:(ncol(train_data) - 1)]

valid_data <- valid_data[, c(setdiff(names(valid_data), coluna), coluna)]

features_continuas <- c("Consumo.Sal", "Duracao.Sono", "IMC")
features_categoricas <- c(
  "Historico.Familiar",
  "Fumante",
  "Nivel.de.Estresse",
  
  "Historico.Pressao.ArterialHipertensao",
  "Historico.Pressao.ArterialNormal",
  "Historico.Pressao.ArterialPre_hipertensao",
  
  "Nivel.ExercicioBaixo",
  "Nivel.ExercicioModerado",
  "Nivel.ExercicioAlto"
)
hypothesis <- getHypothesis(features_continuas, features_categoricas, target = "Risco.Hipertensao", 1)
hypothesis

# Treinamento    
model <- glmnet(train_data[,1:12], train_data$Risco.Hipertensao, family = "binomial", standardize=FALSE, maxit=1e+05, alpha=0, lambda=1e-6)
print(model$beta)
print(model$a0)

# Avaliação do Modelo
x_train <- model.matrix(hypothesis, data=train_data)
y_train <- train_data$class

x_valid <- model.matrix(hypothesis, data=valid_data)
y_valid <- valid_data$class

train_pred <- predict(model, newx=x_train[,1:12], type="response")
valid_pred <- predict(model, newx=x_valid[,1:12], type="response")

# De acordo com Threshold = 0.5, atribui valor previsto como verdadeiro ou falso 
train_class_pred <- train_pred
train_class_pred[train_pred >= 0.5] <- 1
train_class_pred[train_pred < 0.5]  <- 0

valid_class_pred <- valid_pred
valid_class_pred[valid_pred >= 0.5] <- 1
valid_class_pred[valid_pred < 0.5]  <- 0

# gera o Loss para as bases de treino e validação
train_losses <- getLoss(train_data$Risco.Hipertensao, train_class_pred)
valid_losses <- getLoss(valid_data$Risco.Hipertensao, valid_class_pred)

cat("Train\n")
print(train_losses)

cat("Valid\n")
print(valid_losses)

# gera matriz de confusão para dados de treino e validação
train_cm <- confusionMatrix(data = as.factor(train_class_pred), reference = as.factor(train_data$Risco.Hipertensao), positive='1')
valid_cm <- confusionMatrix(data = as.factor(valid_class_pred), reference = as.factor(valid_data$Risco.Hipertensao), positive='1')

# Calculando as métricas para o conjunto de treino
train_metrics = calculateMetrics(train_cm$table)

# Calculando as métricas para o conjunto de validação
valid_metrics = calculateMetrics(valid_cm$table)

# Organizando os resultados em um dataframe
results_baseline <- data.frame(
  Train  = train_metrics,
  Valid  = valid_metrics
)

# Formatando os resultados para melhor leitura
results_baseline$Train <- as.numeric(results_baseline$Train)
results_baseline$Valid <- as.numeric(results_baseline$Valid)

# Evitando notação científica na exibição dos resultados
results_baseline$Train <- format(results_baseline$Train, scientific = FALSE)
results_baseline$Valid <- format(results_baseline$Valid, scientific = FALSE)

# Exibindo os resultados
results_baseline
```

2.1 – O modelo baseline foi suficiente para resolver o problema? Quais métricas de avaliação vocês utilizaram para justificar a resposta?
**Resposta:** <!-- Escreva sua resposta abaixo -->
Para o o problema de predizer o risco de hipertensão de uma pessoa, podemos dizer que é melhor termos uma grande quantidade de falsos positivos do que falsos negativos. Ou seja, não devemos deixar passar pessoas com risco, é melhor que o modelo diga que uma pessoa deve se cuidar mais, fazer exercícios, cuidar da alimentação e buscar exames para confirmar o risco de hipertensão, do que sugerir o contrário e a pessoa ter risco. Logo o modelo deve ser avaliado de acordo com a métrica que é penalizada pela quantidade de falsos negativos, que é o Recall.
Levando isso em consideração, no conjunto de treinamento tivemos cerca de 95,5% de recall, o que é um ótimo resultado, pois estamos deixando de mostrar o risco de hipertensão para apenas 4,5% das pessoas. Foi observado um percentual de 96% na validação, o que nos permite inferir a presença overfitting  no modelo. Para as demais métricas o resultado no conjunto de validação foi melhor do que no de treino, o que nos permite confirmas ocorrência do overfitting. A precisão ficou em torno de 55%, mas não vamos nos precupar com essa métrica, pois ela penaliza a presença de falsos positivos - e como dito anteriormente, não temos que temer dizer que uma pessoa tem risco quando na verdade não tem.
Com esse desempenho o modelo se mostra capaz de prever bem os casos possível de hipertensão, mas ainda podemos melhorar o modelo para que deixe de estar em overfitting. Além disso, embora não seja o nosso objetivo, podemos melhorar a precisão do modelo, para que passemos a termos menos falsos positivos (o que penaliza a precisão do modelo). 
<!-- Fim da resposta -->

# Tarefa 3 -- Soluções alternativas da Regressão Logística

Implementem soluções alternativas baseadas em regressão logística, seja através da combinação das *features* **ou** de modelos polinomiais para melhorar o resultado do *baseline*. Comparem suas soluções reportando métricas de interesse no **conjunto de validação**.

# Analisando correlação entre features para features continuas

```{r}
# Calculando a matriz de correlação
matriz_correlacao <- cor(train_data[,1:5][ , 1:(ncol(train_data[,1:5]) - 1)])

# Ajusta o tamanho do texto para todos os elementos do gráfico
par(cex = 0.8)            

# Plota a matriz de correlação diretamente
corrplot(matriz_correlacao, 
         method = "color", 
         type = "upper",
         addCoef.col = "black", 
         tl.col = "black", 
         tl.srt = 45,           # ângulo dos labels
         number.cex = 0.8)      # tamanho do texto dos coeficientes


```
# Analisando correlação entre features para features discretas

```{r}
# Instalar o pacote se ainda não tiver
# install.packages("vcd")

categoricas <- dados_treino[, c("Fumante", "Nivel.Exercicio", "Nivel.de.Estresse", "Historico.Familiar","Historico.Pressao.Arterial","Risco.Hipertensao")]

# Função para calcular Cramér's V entre duas variáveis
cramers_v <- function(x, y) {
  tbl <- table(x, y)
  assocstats(tbl)$cramer
}

# Matriz de Cramér's V
matriz_cramer <- outer(1:ncol(categoricas), 1:ncol(categoricas), 
                       Vectorize(function(i,j) cramers_v(categoricas[,i], categoricas[,j])))
colnames(matriz_cramer) <- colnames(categoricas)
rownames(matriz_cramer) <- colnames(categoricas)

matriz_cramer

```


```{r atv3-code}
# Hipóteses alternativas - polinomial
hypothesis_grau1 <- getHypothesis(features_continuas, features_categoricas, target = "Risco.Hipertensao",1)
hypothesis_grau1

hypothesis_grau2 <- getHypothesis(features_continuas, features_categoricas, target = "Risco.Hipertensao",2)
hypothesis_grau2

hypothesis_grau3 <- getHypothesis(features_continuas, features_categoricas, target = "Risco.Hipertensao",3)
hypothesis_grau3

hypothesis_grau4 <- getHypothesis(features_continuas, features_categoricas, target = "Risco.Hipertensao",4)
hypothesis_grau4

hypothesis_grau5 <- getHypothesis(features_continuas, features_categoricas, target = "Risco.Hipertensao",5)
hypothesis_grau5

hypothesis_grau6 <- getHypothesis(features_continuas, features_categoricas, target = "Risco.Hipertensao",6)
hypothesis_grau6

# Treinamento
formulas <- c(hypothesis_grau1, hypothesis_grau2, hypothesis_grau3, hypothesis_grau4, hypothesis_grau5, hypothesis_grau6)

PrecisionPorGraus <- data.frame(graus=numeric(6), TrainLoss=numeric(6), ValidLoss=numeric(6))
RecallPorGraus <- data.frame(graus=numeric(6), TrainLoss=numeric(6), ValidLoss=numeric(6))
LossPorGraus <- data.frame(graus=numeric(6), TrainLoss=numeric(6), ValidLoss=numeric(6))
F1PorGraus  <- data.frame(graus=numeric(6), TrainF1=numeric(6), ValidF1=numeric(6))

i <- 1
for(hypothesis in formulas){  
  
    # Preparar os dados de treinamento e validação
  x_train <- model.matrix(hypothesis, train_data)
  y_train <- train_data$class
  
  x_valid <- model.matrix(hypothesis, valid_data)
  y_valid <- valid_data$class
  
  # Treinamento do modelo
  model <- glmnet(train_data[,1:12], train_data$Risco.Hipertensao, family = "binomial", standardize=FALSE, maxit=1e+05, alpha=0, lambda=1e-6)
  
  # Predicao do modelo
  train_pred <- predict(model, newx=x_train[,1:12], type="response")
  valid_pred <- predict(model, newx=x_valid[,1:12], type="response")
  
  # Transformaçao das predicoes em classes
  train_class_pred                    <- train_pred
  train_class_pred[train_pred >= 0.5] <- 1
  train_class_pred[train_pred < 0.5]  <- 0
  
  valid_class_pred                    <- valid_pred
  valid_class_pred[valid_pred >= 0.5] <- 1
  valid_class_pred[valid_pred < 0.5]  <- 0
  
  # Calcula a loss dos dois conjuntos
  train_losses <- getLoss(train_data$Risco.Hipertensao, train_pred)
  valid_losses <- getLoss(valid_data$Risco.Hipertensao, valid_pred)

  
  # Calcula a matriz de confusão
  train_cm <- confusionMatrix(data=as.factor(train_class_pred), 
                        reference=as.factor(train_data$Risco.Hipertensao), 
                        positive='1')
  
  valid_cm <- confusionMatrix(data=as.factor(valid_class_pred), 
                        reference=as.factor(valid_data$Risco.Hipertensao), 
                        positive='1')
  
  # Calcula mẽtrica de interesse
  train_metrics = calculateMetrics(train_cm$table)
  valid_metrics = calculateMetrics(valid_cm$table)
  
  
  PrecisionPorGraus[i,]  <- c(i, train_metrics["Precision"], valid_metrics["Precision"]) 
  RecallPorGraus[i,]  <- c(i, train_metrics["Recall"], valid_metrics["Recall"]) 
  LossPorGraus[i,] <- c(i, train_losses["AverageLoss"], valid_losses["AverageLoss"])
  F1PorGraus[i,]  <- c(i, train_metrics["F1"], valid_metrics["F1"]) 
  
  i <- i + 1
}
# Curva de viés e variância
# Gráfico da Precisão
PrecisionPorGrausMelt <- melt(PrecisionPorGraus, id="graus") 
p <- ggplot(data=PrecisionPorGrausMelt, aes(x=graus, y=value, colour=variable)) + geom_line() + geom_point()

p <- p + ggtitle("Curva vies/variancia") + ylab("Precision") + scale_x_discrete(name ="graus", limits=as.character(1:length(formulas)))

p + theme(legend.position = c(0.7, 0.50), legend.title = element_blank())

# Gráfico da Recall
RecallPorGrausMelt <- melt(RecallPorGraus, id="graus") 
p <- ggplot(data=RecallPorGrausMelt, aes(x=graus, y=value, colour=variable)) + geom_line() + geom_point()

p <- p + ggtitle("Curva vies/variancia") + ylab("Recall") + scale_x_discrete(name ="graus", limits=as.character(1:length(formulas)))

p + theme(legend.position = c(0.7, 0.50), legend.title = element_blank())

# Gráfico da Loss
LossPorGrausMelt <- melt(LossPorGraus, id="graus") 
p <- ggplot(data=LossPorGrausMelt, aes(x=graus, y=value, colour=variable)) + geom_line() + geom_point()

p <- p + ggtitle("Curva vies/variancia") + ylab("Loss") + scale_x_discrete(name ="graus", limits=as.character(1:length(formulas)))

p + theme(legend.position = c(0.7, 0.50), legend.title = element_blank())

# Gráfico da Acurácia
F1PorGrausMelt <- melt(F1PorGraus, id="graus")  # convert to long format
p <- ggplot(data=F1PorGrausMelt, aes(x=graus, y=value, colour=variable)) + geom_line() + geom_point()

p <- p + ggtitle("Curva vies/variancia") + ylab("F1") + scale_x_discrete(name ="graus", limits=as.character(1:length(formulas)))

p + theme(legend.position = c(0.7, 0.50), legend.title = element_blank())
```





```{r atv3-code}
# Hipóteses alternativas - polinomial
hypothesis_grau1 <- getHypothesis(features_continuas, features_categoricas, target = "Risco.Hipertensao",1)
hypothesis_grau1

hypothesis_grau2 <- getHypothesis(features_continuas, features_categoricas, target = "Risco.Hipertensao",2)
hypothesis_grau2

hypothesis_grau3 <- getHypothesis(features_continuas, features_categoricas, target = "Risco.Hipertensao",3)
hypothesis_grau3

hypothesis_grau4 <- getHypothesis(features_continuas, features_categoricas, target = "Risco.Hipertensao",4)
hypothesis_grau4

hypothesis_grau5 <- getHypothesis(features_continuas, features_categoricas, target = "Risco.Hipertensao",5)
hypothesis_grau5

hypothesis_grau6 <- getHypothesis(features_continuas, features_categoricas, target = "Risco.Hipertensao",6)
hypothesis_grau6

# Treinamento
formulas <- c(hypothesis_grau1, hypothesis_grau2, hypothesis_grau3, hypothesis_grau4, hypothesis_grau5, hypothesis_grau6)

PrecisionPorGraus <- data.frame(graus=numeric(6), Precision=numeric(6), Precision=numeric(6))
RecallPorGraus <- data.frame(graus=numeric(6), Recall=numeric(6), Recall=numeric(6))
LossPorGraus <- data.frame(graus=numeric(6), TrainLoss=numeric(6), ValidLoss=numeric(6))
F1PorGraus  <- data.frame(graus=numeric(6), TrainF1=numeric(6), ValidF1=numeric(6))

i <- 1
for(hypothesis in formulas){  
  
    # Preparar os dados de treinamento e validação
  x_train <- model.matrix(hypothesis, train_data)
  y_train <- train_data$class
  
  x_valid <- model.matrix(hypothesis, valid_data)
  y_valid <- valid_data$class
  
  # Treinamento do modelo
  model <- glmnet(train_data[,1:12], train_data$Risco.Hipertensao, family = "binomial", standardize=FALSE, maxit=1e+05, alpha=0, lambda=1e-6)
  
  # Predicao do modelo
  train_pred <- predict(model, newx=x_train[,1:12], type="response")
  valid_pred <- predict(model, newx=x_valid[,1:12], type="response")
  
  # Transformaçao das predicoes em classes
  train_class_pred                    <- train_pred
  train_class_pred[train_pred >= 0.5] <- 1
  train_class_pred[train_pred < 0.5]  <- 0
  
  valid_class_pred                    <- valid_pred
  valid_class_pred[valid_pred >= 0.5] <- 1
  valid_class_pred[valid_pred < 0.5]  <- 0
  
  # Calcula a loss dos dois conjuntos
  train_losses <- getLoss(train_data$Risco.Hipertensao, train_pred)
  valid_losses <- getLoss(valid_data$Risco.Hipertensao, valid_pred)

  
  # Calcula a matriz de confusão
  train_cm <- confusionMatrix(data=as.factor(train_class_pred), 
                        reference=as.factor(train_data$Risco.Hipertensao), 
                        positive='1')
  
  valid_cm <- confusionMatrix(data=as.factor(valid_class_pred), 
                        reference=as.factor(valid_data$Risco.Hipertensao), 
                        positive='1')
  
  # Calcula métrica de interesse
  train_metrics = calculateMetrics(train_cm$table)
  valid_metrics = calculateMetrics(valid_cm$table)
  
  
  PrecisionPorGraus[i,]  <- c(i, train_metrics["Precision"], valid_metrics["Precision"]) 
  RecallPorGraus[i,]  <- c(i, train_metrics["Recall"], valid_metrics["Recall"]) 
  LossPorGraus[i,] <- c(i, train_losses["AverageLoss"], valid_losses["AverageLoss"])
  F1PorGraus[i,]  <- c(i, train_metrics["F1"], valid_metrics["F1"]) 
  
  i <- i + 1
}
# Curva de viés e variância
# Gráfico da Precisão
PrecisionPorGrausMelt <- melt(PrecisionPorGraus, id="graus") 
p <- ggplot(data=PrecisionPorGrausMelt, aes(x=graus, y=value, colour=variable)) + geom_line() + geom_point()

p <- p + ggtitle("Curva vies/variancia") + ylab("Precision") + scale_x_discrete(name ="graus", limits=as.character(1:length(formulas)))

p + theme(legend.position = c(0.7, 0.50), legend.title = element_blank())

# Gráfico da Recall
RecallPorGrausMelt <- melt(RecallPorGraus, id="graus") 
p <- ggplot(data=RecallPorGrausMelt, aes(x=graus, y=value, colour=variable)) + geom_line() + geom_point()

p <- p + ggtitle("Curva vies/variancia") + ylab("Recall") + scale_x_discrete(name ="graus", limits=as.character(1:length(formulas)))

p + theme(legend.position = c(0.7, 0.50), legend.title = element_blank())

# Gráfico da Loss
LossPorGrausMelt <- melt(LossPorGraus, id="graus") 
p <- ggplot(data=LossPorGrausMelt, aes(x=graus, y=value, colour=variable)) + geom_line() + geom_point()

p <- p + ggtitle("Curva vies/variancia") + ylab("Loss") + scale_x_discrete(name ="graus", limits=as.character(1:length(formulas)))

p + theme(legend.position = c(0.7, 0.50), legend.title = element_blank())

# Gráfico da Acurácia
F1PorGrausMelt <- melt(F1PorGraus, id="graus")  # convert to long format
p <- ggplot(data=F1PorGrausMelt, aes(x=graus, y=value, colour=variable)) + geom_line() + geom_point()

p <- p + ggtitle("Curva vies/variancia") + ylab("F1") + scale_x_discrete(name ="graus", limits=as.character(1:length(formulas)))

p + theme(legend.position = c(0.7, 0.50), legend.title = element_blank())

```

3.1 – Quais foram os critérios utilizados para selecionar a técnica alternativa?
Fizemos a análise de correlação entre as features contínuas e as discretas. Nas duas não foi possível observar uma forte correlação entre 2 ou mais features. Por isso não pudemos agrupar e assim reduzir o total de features do modelo. Com isso, combinamos as features contínuas para treinarmos o modelo polinomial.

3.2 – Alguma combinação gerou um impacto positivo ou negativo significativo no desempenho do modelo? Por quê?
Ao aumentar o grau do polinomio de 1 para 2 notamos o valor do Recall caiu, mas já foi possível observar que o modelo saiu de uma região de overfitting. 

3.3 – Existe alguma relação entre viés e variância no seu modelo ao modificar o modelo (seja por combinações de *features* ou polinomiais)?
Para todas as métricas, em exceção o recall, foi possível notar que ao aumentar o grau do polinomio os modelos não conseguiram sair da condição de overfitting.
**Resposta:** <!-- Escreva sua resposta abaixo -->

<!-- Fim da resposta -->

# Tarefa 4 -- Regularização

Escolham um dos modelos do item anterior e variem o fator de regularização ($\lambda$). Avaliem a curva de viés e variância e reportem a matriz de confusão relativa para o melhor modelo no conjunto de validação.

```{r atv4-code}
# Busca de hiperparâmetros
lambda_values <- c(0.1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6)

PrecisionPerRegularizationRigde <- data.frame(regularization=numeric(length(lambda_values)), 
                                    TrainPrecision=numeric(length(lambda_values)),
                                    ValPrecision=numeric(length(lambda_values)))
RecallPerRegularizationRigde <- data.frame(regularization=numeric(length(lambda_values)), 
                                    TrainRecall=numeric(length(lambda_values)),
                                    ValRecall=numeric(length(lambda_values)))
LossPerRegularizationRigde <- data.frame(regularization=numeric(length(lambda_values)), 
                                    TrainLoss=numeric(length(lambda_values)),
                                    ValLoss=numeric(length(lambda_values)))

F1PerRegularizationRigde <- data.frame(regularization=numeric(length(lambda_values)), 
                                   TrainF1=numeric(length(lambda_values)),
                                   ValF1=numeric(length(lambda_values)))

# Treinamento
i <- 1
for(l in lambda_values){  
  
  # Preparar os dados de treinamento e validação
  x_train <- model.matrix(hypothesis, train_data)
  
  x_valid <- model.matrix(hypothesis, valid_data)
  
  # Treinamento do modelo
  model <- glmnet(train_data[,1:12], train_data$Risco.Hipertensao, family = "binomial", standardize=FALSE, maxit=1e+05, alpha=0, lambda=l)
  
  # Predicao do modelo
  train_pred <- predict(model, newx=x_train[,1:12], type="response")
  valid_pred <- predict(model, newx=x_valid[,1:12], type="response")
  
  # Transformaçao das predicoes em classes
  train_class_pred                    <- train_pred
  train_class_pred[train_pred >= 0.5] <- 1
  train_class_pred[train_pred < 0.5]  <- 0
  
  valid_class_pred                    <- valid_pred
  valid_class_pred[valid_pred >= 0.5] <- 1
  valid_class_pred[valid_pred < 0.5]  <- 0
  
  # Calcula a loss dos dois conjuntos
  train_losses <- getLoss(train_data$Risco.Hipertensao, train_pred)
  valid_losses <- getLoss(valid_data$Risco.Hipertensao, valid_pred)

  
  # Calcula a matriz de confusão
  train_cm <- confusionMatrix(data=as.factor(train_class_pred), 
                        reference=as.factor(train_data$Risco.Hipertensao), 
                        positive='1')
  
  valid_cm <- confusionMatrix(data=as.factor(valid_class_pred), 
                        reference=as.factor(valid_data$Risco.Hipertensao), 
                        positive='1')
  
  # Calcula mẽtrica de interesse
  train_metrics = calculateMetrics(train_cm$table)
  valid_metrics = calculateMetrics(valid_cm$table)

  PrecisionPerRegularizationRigde[i,] <- c(i, train_metrics["Precision"], valid_metrics["Precision"])
  RecallPerRegularizationRigde[i,] <- c(i, train_metrics["Recall"], valid_metrics["Recall"])
  LossPerRegularizationRigde[i,] <- c(i, train_losses["AverageLoss"], valid_losses["AverageLoss"])
  F1PerRegularizationRigde[i,]  <- c(i, train_metrics["F1"], valid_metrics["F1"]) 
  
  i <- i + 1
}
```

```{r}
# Gráfico da Precisão
PrecisionPerRegularizationRigdeMelt <- melt(PrecisionPerRegularizationRigde, id="regularization") 
p <- ggplot(data=PrecisionPerRegularizationRigdeMelt, aes(x=regularization, y=value, colour=variable)) + geom_line() + geom_point()

p <- p + ggtitle("Curva vies/variancia - Ridge") + ylab("Precisão") + scale_x_discrete(name ="Parametro de regularizacao", limits=c("0.1", "1e-2", "1e-3", "1e-4", "1e-5", "1e-6"))

p + theme(legend.position = c(0.7, 0.50), legend.title = element_blank())

# Gráfico do Recall
RecallPerRegularizationRigdeMelt <- melt(RecallPerRegularizationRigde, id="regularization") 
p <- ggplot(data=RecallPerRegularizationRigdeMelt, aes(x=regularization, y=value, colour=variable)) + geom_line() + geom_point()

p <- p + ggtitle("Curva vies/variancia - Ridge") + ylab("Recall") + scale_x_discrete(name ="Parametro de regularizacao", limits=c("0.1", "1e-2", "1e-3", "1e-4", "1e-5", "1e-6"))

p + theme(legend.position = c(0.7, 0.50), legend.title = element_blank())

# Gráfico da Loss
LossPerRegularizationMelt <- melt(LossPerRegularizationRigde, id="regularization") 
p <- ggplot(data=LossPerRegularizationMelt, aes(x=regularization, y=value, colour=variable)) + geom_line() + geom_point()

p <- p + ggtitle("Curva vies/variancia - Ridge") + ylab("Loss") + scale_x_discrete(name ="Parametro de regularizacao", limits=c("0.1", "1e-2", "1e-3", "1e-4", "1e-5", "1e-6"))

p + theme(legend.position = c(0.7, 0.50), legend.title = element_blank())

# Curva de viés e variância
F1PerRegularizationMelt <- melt(F1PerRegularizationRigde, id="regularization")
p <- ggplot(data=F1PerRegularizationMelt, aes(x=regularization, y=value, colour=variable)) + geom_line() + geom_point()

p <- p + ggtitle("Curva vies/variancia - Ridge") + ylab("F1 Balanced") + scale_x_discrete(name ="Parametro de regularizacao", limits=c("0.1", "1e-2", "1e-3", "1e-4", "1e-5", "1e-6"))

p + theme(legend.position = c(0.7, 0.50), legend.title = element_blank())

# Métricas de avaliação
```

4.1 – Como a regularização afetou o desempenho do modelo?
**Resposta:** <!-- Escreva sua resposta abaixo -->
A regularização impactou positivamente o modelo, especialmente no Recall, métrica mais relevante para nosso problema. Com o parâmetro de 1e-3, obtivemos Recall de 0.9421, valor próximo ao do modelo inicial, mas agora com maior evidência de ausência de overfitting, pois o desempenho no conjunto de treinamento se manteve equilibrado em relação ao de validação.

<!-- Fim da resposta -->

4.2 – Foi utilizada regularização Lasso ou Ridge? Por quê?
**Resposta:** <!-- Escreva sua resposta abaixo -->
Optamos pela regularização Ridge, pois os modelos anteriores apresentavam sinais de overfitting, e o Ridge ajuda a reduzir a variância sem zerar coeficientes, preservando informações das features.

<!-- Fim da resposta -->

4.3 – Houve um ponto ótimo onde o erro foi minimizado? Como vocês identificaram esse ponto?
**Resposta:** <!-- Escreva sua resposta abaixo -->
Com o parâmetro de 1e-5, o Recall apresentou melhora, indicando redução do erro nessa métrica. No entanto, esse ponto já se situa em uma região de overfitting, evidenciada pelo aumento do desempenho no treino em relação à validação.

<!-- Fim da resposta -->

4.4 – Em quais regiões da curva de viés e variância foi possível identificar underfitting e overfitting?
**Resposta:** <!-- Escreva sua resposta abaixo -->
Observando o F1 e o Loss, percebemos que ao reduzir o parâmetro de regularização de 1e-3 para 1e-4, a diferença entre treino e validação aumenta significativamente, caracterizando overfitting. Antes de 1e-3, o modelo apresentava underfitting, enquanto para o Recall, entre 1e-3 e 1e-4, a validação se aproxima do desempenho do treino, reforçando a identificação dessas regiões.

<!-- Fim da resposta -->

# Tarefa 5 -- Árvore de Decisão

Treinem um modelo *baseline 2* de árvore de decisão **todas as features disponíveis**.

```{r atv5-code}
# Treinamento    

# não precisamos de one-hot encoding para treinar a árvore de decisão
summary(treino_normalizado)

baseline2 <- rpart(Risco.Hipertensao ~ Consumo.Sal + Nivel.de.Estresse + 
                      Historico.Pressao.Arterial + Duracao.Sono + IMC +
                      Historico.Familiar + Nivel.Exercicio + Fumante, 
                    data=treino_normalizado,
                    method="class",
                    control=rpart.control(minsplit=2, cp=0.0, xval=10),
                    parms=list(split="gini"))

# Avaliação do Modelo 

print(baseline2$cptable)
```
Avaliando o modelo Baseline 2 (árvore de decisão):


```{r}
# Predicao do modelo

getModelMetrics <- function(model, training_set, validation_set){
  model_train_pred <- predict(model, training_set, type="class")
  model_valid_pred <- predict(model, validacao_normalizado, type="class")

  # Calcula a matriz de confusão
  train_cm <- confusionMatrix(data=as.factor(model_train_pred), 
                        reference=as.factor(training_set$Risco.Hipertensao), 
                        positive='1')
  
  valid_cm <- confusionMatrix(data=as.factor(model_valid_pred), 
                        reference=as.factor(validation_set$Risco.Hipertensao), 
                        positive='1')
  
  
  # Calculando as métricas para o conjunto de treino
  train_metrics = calculateMetrics(train_cm$table)
  
  # Calculando as métricas para o conjunto de validação
  valid_metrics = calculateMetrics(valid_cm$table)
  
  # Organizando os resultados em um dataframe
  results <- data.frame(
    Train  = train_metrics,
    Valid  = valid_metrics
  )
  
  # Formatando os resultados para melhor leitura
  results$Train <- as.numeric(results$Train)
  results$Valid <- as.numeric(results$Valid)
  
  # Evitando notação científica na exibição dos resultados
  results$Train <- format(results$Train, scientific = FALSE)
  results$Valid <- format(results$Valid, scientific = FALSE)
  
  # Exibindo os resultados
  list(train_cm=getRelativeConfusionMatrix(train_cm),
       valid_cm=getRelativeConfusionMatrix(valid_cm),
       metrics=results
  )
}


baseline2_metrics <- getModelMetrics(baseline2, treino_normalizado, validacao_normalizado)
baseline2_metrics$train_cm
baseline2_metrics$valid_cm
baseline2_metrics$metrics
```



5.1 – O modelo *baseline 2* foi suficiente para resolver o problema? Quais métricas de avaliação vocês utilizaram para justificar a resposta?

**Resposta:** <!-- Escreva sua resposta abaixo -->

O modelo baseline 2 parece bom no conjunto de validação, pois atinge uma Acurácia Balanceada de 84,5%. No entanto, ao comparar com o erro de treinamento e considerando também a profundidade e quantidade de nós no modelo baseline 2, tudo indica que estamos na realidade num cenário de overfitting, e a performance no mundo real poderia ser bem menor.

<!-- Fim da resposta -->

# Tarefa 6 -- Profundidade da Árvore de Decisão

Treinem árvores de decisão variando a profundidade máxima. Mostrem a curva de viés e variância, no qual o eixo X é a profundidade e o eixo Y é o erro ou uma métrica de interesse.

```{r atv6-code}
# Busca de hiperparâmetros

depthSearch <- function(formula, training_set, validation_set, max_depth = 20){
  depths <- 1:20
  
  AccPerDepth  <- data.frame(depth=length(depths), 
                             TrainAcc=length(depths), ValidAcc=length(depths))
  F1PerDepth  <- data.frame(depth=length(depths), 
                            TrainF1=length(depths), ValidF1=length(depths))
  RecallPerDepth  <- data.frame(depth=length(depths), 
                              TrainRec=length(depths), ValidRec=length(depths))
  PrecisionPerDepth  <- data.frame(depth=length(depths), 
                            TrainPrec=length(depths), ValidPrec=length(depths))


  for(i in depths){
    depth_tree_model <- rpart(formula, 
                      data=training_set,
                      method="class",
                      control=rpart.control(minsplit=2, cp=0.0, xval=10, maxdepth=i),
                      parms=list(split="gini"))
    
    depth_train_pred <- predict(depth_tree_model, training_set, type="class")
    depth_valid_pred <- predict(depth_tree_model, validation_set, type="class")
  
    # Calcula a matriz de confusão
    depth_train_cm <- confusionMatrix(data=as.factor(depth_train_pred), 
                          reference=as.factor(training_set$Risco.Hipertensao), 
                          positive='1')
    
    depth_valid_cm <- confusionMatrix(data=as.factor(depth_valid_pred), 
                          reference=as.factor(validation_set$Risco.Hipertensao), 
                          positive='1')
    
    
    # Calculando as métricas para o conjunto de treino
    depth_train_metrics = calculateMetrics(depth_train_cm$table)
    
    # Calculando as métricas para o conjunto de validação
    depth_valid_metrics = calculateMetrics(depth_valid_cm$table)
    
    AccPerDepth[i,]  <- c(i, depth_train_metrics["BalAcc"], depth_valid_metrics["BalAcc"])
    F1PerDepth[i,]  <- c(i, depth_train_metrics["F1"], depth_valid_metrics["F1"])
    RecallPerDepth[i,]  <- c(i, depth_train_metrics["Recall"], depth_valid_metrics["Recall"])
    PrecisionPerDepth[i,]  <- c(i, depth_train_metrics["Precision"], depth_valid_metrics["Precision"])
  }
  list(BalAccuracy=AccPerDepth, Precision=PrecisionPerDepth, Recall=RecallPerDepth, F1=F1PerDepth)
}


metricsCurve <- depthSearch(
              formula = Risco.Hipertensao ~ Consumo.Sal + Nivel.de.Estresse + 
                        Historico.Pressao.Arterial + Duracao.Sono + IMC +
                        Historico.Familiar + Nivel.Exercicio + Fumante,
              treino_normalizado,
              validacao_normalizado)

# Treinamento

# Curva de viés e variância

AccPerDepthMelt <- melt(metricsCurve$BalAccuracy, id="depth")
p_acc <- ggplot(data=AccPerDepthMelt, aes(x=depth, y=value, colour=variable)) + geom_line() + geom_point()

p_acc <- p_acc + ggtitle("Curva vies/variancia") + ylab("Acurácia Balanceada") + scale_x_discrete(name="Profundidade", limits=as.character(1:20))

p_acc + theme(legend.position = c(0.2, 0.1), legend.title = element_blank())

F1PerDepthMelt <- melt(metricsCurve$F1, id="depth")
p_f1 <- ggplot(data=F1PerDepthMelt, aes(x=depth, y=value, colour=variable)) + geom_line() + geom_point()

p_f1 <- p_f1 + ggtitle("Curva vies/variancia") + ylab("F1-Score") + scale_x_discrete(name="Profundidade", limits=as.character(1:20))

p_f1 + theme(legend.position = c(0.2, 0.1), legend.title = element_blank())


# Métricas de avaliação

```
Matriz de confusão do melhor modelo por profundidade

```{r}


optimal_depth_tree_model <- rpart(Risco.Hipertensao ~ Consumo.Sal + 
                      Nivel.de.Estresse + Historico.Pressao.Arterial + 
                      Duracao.Sono + IMC + Historico.Familiar + 
                      Nivel.Exercicio + Fumante, 
                    data=treino_normalizado,
                    method="class",
                    control=rpart.control(minsplit=2, cp=0.0, xval=10, maxdepth=5),
                    parms=list(split="gini"))
  
optimal_depth_tree_metrics <- getModelMetrics(optimal_depth_tree_model, treino_normalizado, validacao_normalizado)
optimal_depth_tree_metrics$train_cm
optimal_depth_tree_metrics$valid_cm
optimal_depth_tree_metrics$metrics
```

6.1 – Como a profundidade afetou o desempenho do modelo?

**Resposta:** <!-- Escreva sua resposta abaixo -->

Conforme aumentamos a profundidade máxima permitida para a árvore, vimos a acurácia no conjunto de treinamento aumentar continuamente, acompanhada da acurácia de validação. Este comportamento persiste até por volta da profundidade 5, aonde notamos um afastamento constante da acurácia de treinamento, que chega a 1 para a de validação, que gravita em torno do valor 0.85, sugerindo overfitting.

<!-- Fim da resposta -->

6.2 – Houve um ponto ótimo onde o erro foi minimizado? Como vocês identificaram esse ponto?

**Resposta:** <!-- Escreva sua resposta abaixo -->

Ao comparar as curvas para o F1-score e Acurácia Balanceada, vemos os comportamentos se repetindo, onde ambas as métricas melhoram até a profundidade 5 e, a partir disso, a métrica na validação se estabiliza enquanto a métrica de treinamento continua aumentando até 100%, sugerindo que 5 seria uma profundidade ideal para não ter underfitting nem overfitting.

<!-- Fim da resposta -->

6.3 – Em quais regiões da curva de viés e variância foi possível identificar *underfitting* e *overfitting*?

**Resposta:** <!-- Escreva sua resposta abaixo -->

O comportamento das curvas sugere que nas profundidades 1, 2 e 3 estaríamos ainda em underfitting, pois a métrica aumenta consideravelmente de um passo para o outro. Entre os niveis 4 e 5 já vemos uma mudança mais estável na métrica, sugerindo um fit ideal, com 5 sendo o ponto máximo, aonde poderíamos ter a melhor performance no mundo real (métrica de validação máxima). Da profundidade 5 em diante, já estamos em overfitting, pois nota-se uma queda acentuada na métrica de validação, com aumento constante da métrica de treino.

<!-- Fim da resposta -->

# Tarefa 7 -- Soluções alternativas da Árvore de Decisão

Explorem pelo menos dois subconjuntos de *features* para treinar diferentes árvores de decisão.

**Dica: observem a importância de cada *feature* por meio do atributo *variable.importance*.**

```{r atv7-code}
print(baseline2$variable.importance)

# Hipóteses alternativas

alternativo_5melhores_formula <- Risco.Hipertensao ~ Consumo.Sal + Nivel.de.Estresse + 
                  Historico.Pressao.Arterial + Duracao.Sono + IMC

alternativo_3melhores_formula <- Risco.Hipertensao ~ Consumo.Sal + 
              Historico.Pressao.Arterial + Duracao.Sono

alternativo_5piores_formula <- Risco.Hipertensao ~ IMC + Nivel.Exercicio +
              Nivel.de.Estresse + Fumante + Historico.Familiar

# Treinamento


alternativo_5melhores_model <- rpart(alternativo_5melhores_formula, 
                    data=treino_normalizado,
                    method="class",
                    control=rpart.control(minsplit=2, cp=0.0, xval=10, maxdepth=10),
                    parms=list(split="gini"))

alternativo_3melhores_model <- rpart(alternativo_3melhores_formula, 
                    data=treino_normalizado,
                    method="class",
                    control=rpart.control(minsplit=2, cp=0.0, xval=10, maxdepth=10),
                    parms=list(split="gini"))

alternativo_5piores_model <- rpart(alternativo_5piores_formula, 
                    data=treino_normalizado,
                    method="class",
                    control=rpart.control(minsplit=2, cp=0.0, xval=10, maxdepth=10),
                    parms=list(split="gini"))

```

Calculando métricas dos modelos alternativos


```{r}

alternativo_5melhores_metrics <- getModelMetrics(alternativo_5melhores_model, treino_normalizado, validacao_normalizado)
alternativo_5melhores_metrics$train_cm
alternativo_5melhores_metrics$valid_cm
alternativo_5melhores_metrics$metrics

```
```{r}
alternativo_3melhores_metrics <- getModelMetrics(alternativo_3melhores_model, treino_normalizado, validacao_normalizado)
alternativo_3melhores_metrics$train_cm
alternativo_3melhores_metrics$valid_cm
alternativo_3melhores_metrics$metrics

```
```{r}

alternativo_5piores_metrics <- getModelMetrics(alternativo_5piores_model, treino_normalizado, validacao_normalizado)
alternativo_5piores_metrics$train_cm
alternativo_5piores_metrics$valid_cm
alternativo_5piores_metrics$metrics

```

Determinado que o modelo Alternativo com as 5 features mais importantes foi o que teve melhor performance na Acurácia Balanceada, exploraremos as variações do hiperparâmetro maxdepth neste mesmo modelo:

```{r}


metricsCurve_5melhores <- depthSearch(
              formula = alternativo_5melhores_formula,
              treino_normalizado,
              validacao_normalizado)

# Treinamento

# Curva de viés e variância

AccPerDepthMelt <- melt(metricsCurve$BalAccuracy, id="depth")
p_acc <- ggplot(data=AccPerDepthMelt, aes(x=depth, y=value, colour=variable)) + geom_line() + geom_point()

p_acc <- p_acc + ggtitle("Curva vies/variancia") + ylab("Acurácia Balanceada") + scale_x_discrete(name="Profundidade", limits=as.character(1:20))

p_acc + theme(legend.position = c(0.2, 0.1), legend.title = element_blank())

# Métricas de avaliação
```

Novamente, o melhor valor de maxdepth parece ser 5, então faremos uma simulação extra, com este hiperparâmetro calibrado para a melhor performance. 

```{r}

alternativo_5melhores_maxdepth_model <- rpart(alternativo_5melhores_formula, 
                    data=treino_normalizado,
                    method="class",
                    control=rpart.control(minsplit=2, cp=0.0, xval=10, maxdepth=5),
                    parms=list(split="gini"))

alternativo_5melhores_maxdepth_metrics <- getModelMetrics(alternativo_5melhores_maxdepth_model, treino_normalizado, validacao_normalizado)
alternativo_5melhores_maxdepth_metrics$train_cm
alternativo_5melhores_maxdepth_metrics$valid_cm
alternativo_5melhores_maxdepth_metrics$metrics

```

7.1 – Quais foram os critérios utilizados para selecionar a técnica alternativa?

**Resposta:** <!-- Escreva sua resposta abaixo -->

Usamos como critério a importância atribuída pelo processo de construção das árvores de decisão. Considerando essa informação, notamos que a Pressão Arterial é, de longe, o atributo mais importante para 

<!-- Fim da resposta -->

7.2 – Alguma combinação gerou um impacto positivo ou negativo significativo no desempenho do modelo? Por quê?

**Resposta:** <!-- Escreva sua resposta abaixo -->

Todas as seleções alternativas pioraram a performance global do modelo. Em especial, a redução de Acurácia Balanceada mais notável foi a do modelo com as 5 "piores" features, como já era esperado. Com este modelo, tivemos uma redução na métrica de 86%, no modelo ótimo anterior, para 63%. Dentre os modelos alternativos, o que desempenhou melhor foi o com as 5 melhores features, mas sua performance ainda assim foi inferior, atingindo apenas 79% contra os 86% originais, mesmo após calibragem de profundidade máxima.

<!-- Fim da resposta -->

# Tarefa 8 -- Floresta Aleatórias

Treinem *N* florestas aleatórias, variando o número de árvores. Mostrem a curva de viés e variância, na qual o eixo X é a profundidade e o eixo Y é o erro ou uma métrica de interesse.

```{r atv8-code}
# Busca de hiperparâmetros

treino_normalizado_rf <- treino_normalizado
treino_normalizado_rf$Risco.Hipertensao <- as.factor(treino_normalizado_rf$Risco.Hipertensao)

valid_normalizado_rf <- validacao_normalizado
valid_normalizado_rf$Risco.Hipertensao <- as.factor(valid_normalizado_rf$Risco.Hipertensao)

tree_count <- 1:20

rf_acc <- data.frame(trees=length(tree_count), 
                             TrainAcc=length(tree_count), ValidAcc=length(tree_count))

rf_rec <- data.frame(trees=length(tree_count), 
                             TrainRecall=length(tree_count), ValidRecall=length(tree_count))

# Treinamento
for(trees in tree_count){
  rf_model <- randomForest(Risco.Hipertensao ~ ., 
                         data = treino_normalizado_rf, 
                         tree = trees,
                         maxnodes = 21,
                         mtry = 4) # metade da quantidade de features (floor)
  
  rf_train_pred <- predict(rf_model, treino_normalizado_rf, type="class")
  rf_valid_pred <- predict(rf_model, valid_normalizado_rf, type="class")

  # Calcula a matriz de confusão
  rf_train_cm <- confusionMatrix(data=rf_train_pred, 
                        reference=treino_normalizado_rf$Risco.Hipertensao, 
                        positive='1')
  
  rf_valid_cm <- confusionMatrix(data=rf_valid_pred, 
                        reference=valid_normalizado_rf$Risco.Hipertensao, 
                        positive='1')
  
  
  # Calculando as métricas para o conjunto de treino
  rf_train_metrics = calculateMetrics(rf_train_cm$table)
  
  # Calculando as métricas para o conjunto de validação
  rf_valid_metrics = calculateMetrics(rf_valid_cm$table)
  
  rf_acc[trees,]  <- c(trees, rf_train_metrics["BalAcc"], rf_valid_metrics["BalAcc"])
  rf_rec[trees,]  <- c(trees, rf_train_metrics["Recall"], rf_valid_metrics["Recall"])
}

rf_acc_melt <- melt(rf_acc, id="trees")
rf_gg_acc <- ggplot(data=rf_acc_melt, aes(x=trees, y=value, colour=variable)) + geom_line() + geom_point()

rf_gg_acc <- rf_gg_acc + ggtitle("Curva vies/variancia") + ylab("Acurácia Balanceada") + scale_x_discrete(name="# de árvores", limits=as.character(1:20))

rf_gg_acc + theme(legend.position = c(0.2, 0.1), legend.title = element_blank())


rf_rec_melt <- melt(rf_rec, id="trees")
rf_gg_rec <- ggplot(data=rf_rec_melt, aes(x=trees, y=value, colour=variable)) + geom_line() + geom_point()

rf_gg_rec <- rf_gg_rec + ggtitle("Curva vies/variancia") + ylab("Recall") + scale_x_discrete(name="# de árvores", limits=as.character(1:20))

rf_gg_rec + theme(legend.position = c(0.2, 0.1), legend.title = element_blank())

```

Avaliando a matriz de confusão para o modelo ótimo, com 10 árvores:

```{r}

optimal_rf_model <- randomForest(Risco.Hipertensao ~ ., 
                         data = treino_normalizado_rf, 
                         tree = 10,
                         maxnodes = 21,
                         mtry = 4) # metade da quantidade de features (floor)

optimal_depth_tree_metrics <- getModelMetrics(optimal_rf_model, treino_normalizado_rf, valid_normalizado_rf)
optimal_depth_tree_metrics$train_cm
optimal_depth_tree_metrics$valid_cm
optimal_depth_tree_metrics$metrics

```

8.1 – Como a quantidade de árvores afetou o desempenho do modelo?

**Resposta:** <!-- Escreva sua resposta abaixo -->

Ao treinar com o algoritmo padrão do pacote randomForest, notamos que a estratégia parecia estar causando overfitting logo de início, pois a acurácia de treinamento estava fixa em 100%, e com poucas variações na acurácia de validação. Isso era devido ao fato de não haver nenhuma restrição sobre a profundidade de cada árvore da floresta, então caíamos no problema visto no modelo original. Infelizmente, o pacote randomForest não expõe nenhum parametro para controlar diretamente a profundidade, então calculamos a quantidade de nós utilizados pela árvore de profundidade ideal que encontramos e utilizamos este valor (21) como uma aproximação para limitar a profundidade, como gostaríamos. Ao executar essa operação, conseguimos um comportamento mais normal da curva viés/variância, onde temos o número de árvores abaixo de 5 causando uma melhora na acurácia de treinamento, mas sem alterar a validação. Conforme aumentamos o valor, o efeito deste hiperparâmetro nas curvas se torna bem ruidoso.

<!-- Fim da resposta -->

8.2 – Houve um ponto ótimo onde o erro foi minimizado? Como vocês identificaram esse ponto?

**Resposta:** <!-- Escreva sua resposta abaixo -->

Como a curva não segue uma tendência clara, foi um pouco difícil isolar um ponto apenas observando uma métrica. Retomando o argumento utilizado na tarefa 2, resolvemos utilizar o Recall como métrica complementar. Ao comparar as duas curvas, elas parecem sugerir o valor de 10 árvores como ótimo para ambas as métricas, embora essa intensidade varie a cada experimento, dada a natureza aleatória do Random Forest.

<!-- Fim da resposta -->


8.3 – Em quais regiões da curva de viés e variância foi possível identificar *underfitting* e *overfitting*?

**Resposta:** <!-- Escreva sua resposta abaixo -->

Pelo comportamento das curvas, não foi possível isolar regiões muito bem demarcadas, pois tanto a curva de validação quanto de treinamento oscilam bastante juntas, sugerindo que toda a região avaliada é passível de prover um bom fit dos dados.

<!-- Fim da resposta -->


# Tarefa 10 -- Conjunto de Teste

Após identificar o melhor modelo, utilizem-no para fazer previsões no conjunto de teste e avaliem o seu desempenho.

Avaliando uma combinação das 4 métricas de classificação geradas ao longo do trabalho, o modelo completo com profundidade 5 e o modelo de random forest com 10 árvores tem desempenhos muito próximos, com o random forest sendo ligeiramente superior. Escolheremos o modelo Random Forest para utilizar no teste final.


```{r atv10-code}
# Ingestão de arquivo de teste
dados_teste <- read.csv("T02_test_set.csv")

if (any(is.na(dados_teste))) {
  cat("Aviso: Existem valores ausentes no conjunto de teste\n")
} else {
  cat("Nenhum valor ausente encontrado no conjunto de teste\n")
}

# Avaliando dados presentes no data set
# class(dados_teste$Consumo.Sal)
# class(dados_teste$Nivel.de.Estresse)
# class(dados_teste$Historico.Pressao.Arterial)
# class(dados_teste$IMC)
# class(dados_teste$Historico.Familiar)
# class(dados_teste$Nivel.Exercicio)
# class(dados_teste$Fumante)
# class(dados_teste$Risco.Hipertensao)

# summary(dados_teste)

cols <- c(1, 2, 4, 5)  # índices das colunas numericas que vamos normalizar
dados_teste_normalizado <- dados_teste
dados_teste_normalizado[, cols] <- sweep(dados_teste[, cols], 2, min_features, "-")
dados_teste_normalizado[, cols] <- sweep(dados_teste[, cols], 2, diff, "/")

dados_para_encoding_teste <- dados_teste_normalizado

dados_para_encoding_teste$Historico.Pressao.Arterial <- as.factor(dados_para_encoding_teste$Historico.Pressao.Arterial)
one_hot <- model.matrix(~ Historico.Pressao.Arterial - 1, data = dados_para_encoding_teste)
dados_para_encoding_teste <- cbind(dados_para_encoding_teste, one_hot)
dados_para_encoding_teste$Historico.Pressao.Arterial <- NULL
colnames(dados_para_encoding_teste) <- gsub("-", "_", colnames(dados_para_encoding_teste))

dados_para_encoding_teste$Nivel.Exercicio <- as.factor(dados_para_encoding_teste$Nivel.Exercicio)
one_hot <- model.matrix(~ Nivel.Exercicio - 1, data = dados_para_encoding_teste)
dados_para_encoding_teste <- cbind(dados_para_encoding_teste, one_hot)
dados_para_encoding_teste$Nivel.Exercicio <- NULL
colnames(dados_para_encoding_teste) <- gsub("-", "_", colnames(dados_para_encoding_teste))

# Predição no conjunto de teste
# dados_teste_tratados <- dados_para_encoding_teste
# melhor_modelo <- # PREENCHER COM O MELHOR MODELO ENCONTRADO
# resultado_final <- getModelMetrics(melhor_modelo, treino_normalizado, dados_teste_normalizado)

# Métricas de avaliação

# Métricas de avaliação
```

10.1 – Qual foi o critério adotado para definir o melhor modelo?

**Resposta:** <!-- Escreva sua resposta abaixo -->

<!-- Fim da resposta -->

10.2 – Como os resultados no conjunto de teste se comparam aos obtidos no conjunto de validação? Houve algum indício degradação ou melhora do modelo?

**Resposta:** <!-- Escreva sua resposta abaixo -->

<!-- Fim da resposta -->

10.4 – Expliquem a diferença entre os modelos e o porquê que estas diferenças levaram a resultados piores ou melhores.

**Resposta:** <!-- Escreva sua resposta abaixo -->

<!-- Fim da resposta -->

10.5 – Considerando as diferentes abordagens testadas, quais estratégias poderiam ser aplicadas para melhorar ainda mais o desempenho do modelo?

**Resposta:** <!-- Escreva sua resposta abaixo -->

<!-- Fim da resposta -->

# Observações Finais

Justifiquem suas escolhas com base em evidências (métricas, gráficos, hipóteses teóricas).

A clareza na comunicação dos resultados é mais importante do que a qualidade do modelo!
